{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module covers: \n",
    "\n",
    "- Tokenization.\n",
    "- Stemming \n",
    "- Lemmatization\n",
    "    - Lemmatization - NLTK\n",
    "    - Lemmatization - spaCy\n",
    "    - Lemmatization - TFIDF tokenization parameter\n",
    "    - Simple illustration of the dunder call method within a custom class\n",
    "- TFIDF (includes tokenization, lemmatization and stopwords)\n",
    "- Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sharmaa1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Regex\n",
    "import re\n",
    "\n",
    "# Standard modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# POS for nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "\n",
    "# DataFrameMapper\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Random Text for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is not the best text, but it is better than good. \n",
      "It includes some words including good. \n",
      "I could include more but let's not. \n",
      "Live and let live.  \n",
      "The cat jumped over the other cats.Not just another cat.\n",
      "June, the girl, was born in June, the month.\n",
      "Some of the worst months are worse than some of the bad years.\n",
      "Happy New Year, Ashish.\n",
      "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
      "The legislation of the legislature is a weird set of words.\n",
      "\n",
      "------------------------------\n",
      "# of words in text :  87\n",
      "------------------------------\n",
      "# of unique words  :  65\n",
      "------------------------------\n",
      "# of tokens        :  87\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "This is not the best text, but it is better than good. \n",
    "It includes some words including good. \n",
    "I could include more but let's not. \n",
    "Live and let live.  \n",
    "The cat jumped over the other cats.Not just another cat.\n",
    "June, the girl, was born in June, the month.\n",
    "Some of the worst months are worse than some of the bad years.\n",
    "Happy New Year, Ashish.\n",
    "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
    "The legislation of the legislature is a weird set of words.\n",
    "\"\"\"\n",
    "print(text1)\n",
    "\n",
    "list_of_text = (text1.strip()).split(\" \")\n",
    "\n",
    "print('-'*30)\n",
    "print('# of words in text : ', len((text1.strip()).split(\" \")))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of unique words  : ', len(set(list_of_text)))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of tokens        : ', len((text1.strip()).split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all words from a text using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_txt1 = \"Hi New York! What's new and happening and rocking your world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_list_of_words = re.findall(r'\\w+', tmp_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(regex_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " 'What',\n",
       " 's',\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) missing if we simply only extract words\n",
    "- What's --> What and s\n",
    "- Duplicate words included since we are only using a simple Regex command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization - NLTK\n",
    "\n",
    "- Get all words using NLTK word tokens using nltk.tokenize\n",
    "- **`word_tokenize()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words = word_tokenize(tmp_txt1)\n",
    "print(type(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " '!',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words\n",
    "# print(nltk_word_tkn_list_of_words, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) included\n",
    "- What's --> What and 's\n",
    "- Duplicated words included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming - NLTK\n",
    "\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer (English Stemmer, Porter2 Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Porter Stemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "p_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Porter)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fairli         \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Porter)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {p_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x1b818a4b608>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Snowball Stemmer\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "s_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fair           \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {s_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "generous     ---> generous       \n",
      "generously   ---> generous       \n",
      "generation   ---> generat        \n",
      "generate     ---> generat        \n"
     ]
    }
   ],
   "source": [
    "words2 = ['generous', 'generously', 'generation', 'generate']\n",
    "\n",
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word2 in words2:\n",
    "    print(f'{word2:{12}} ---> {s_stemmer.stem(word2):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "employ       ---> employ         \n",
      "employs      ---> employ         \n",
      "employing    ---> employ         \n",
      "employment   ---> employ         \n",
      "employee     ---> employe        \n",
      "employees    ---> employe        \n",
      "employer     ---> employ         \n",
      "employers    ---> employ         \n"
     ]
    }
   ],
   "source": [
    "words3 = ['employ', 'employs', 'employing', 'employment', 'employee', 'employees', 'employer', 'employers']\n",
    "\n",
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word3 in words3:\n",
    "    print(f'{word3:{12}} ---> {s_stemmer.stem(word3):{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization - NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General process for using this is to first tokenize your text (usually token will be word)\n",
    "- Subsequently, use an instance of the **`WordNetLemmatizer()`** and call the lemmatize method on each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easily         \n",
      "fairly       ---> fairly         \n",
      "fairness     ---> fairness       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {lem.lemmatize(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "generous     ---> generous       \n",
      "generously   ---> generously     \n",
      "generation   ---> generation     \n",
      "generate     ---> generate       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word2 in words2:\n",
    "    print(f'{word2:{12}} ---> {lem.lemmatize(word2):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "employ       ---> employ         \n",
      "employs      ---> employ         \n",
      "employing    ---> employing      \n",
      "employment   ---> employment     \n",
      "employee     ---> employee       \n",
      "employees    ---> employee       \n",
      "employer     ---> employer       \n",
      "employers    ---> employer       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word3 in words3:\n",
    "    print(f'{word3:{12}} ---> {lem.lemmatize(word3):{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization dependent on POS of word**\n",
    "- Below shows an example of why it is important to provide a POS tag of the word that will be lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd = 'content'\n",
    "wrd = 'strips'\n",
    "wrd = 'lead'\n",
    "wrd = 'leaves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd, pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leave'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('leaves', 'NNS')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)\n",
    "pos_tag(['leaves'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('No', 'DT'), ('one', 'NN'), ('leaves', 'VBZ'), ('him', 'PRP'), ('like', 'IN'), ('this', 'DT')]\n",
      "------------------------------\n",
      "[('There', 'EX'), ('are', 'VBP'), ('leaves', 'VBZ'), ('all', 'DT'), ('over', 'IN'), ('the', 'DT'), ('lawn', 'NN')]\n",
      "------------------------------\n",
      "[('Dried', 'NNP'), ('leaves', 'VBZ'), ('on', 'IN'), ('trees', 'NNS')]\n",
      "------------------------------\n",
      "[('Employers', 'NNS'), ('can', 'MD'), ('employ', 'VB'), ('employees', 'NNS'), ('or', 'CC'), ('employee', 'NN'), ('for', 'IN'), ('employment', 'NN'), ('or', 'CC'), ('employing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "leaves1 = 'No one leaves him like this'\n",
    "leaves2 = 'There are leaves all over the lawn'\n",
    "leaves3 = 'Dried leaves on trees'\n",
    "emps1 = 'Employers can employ employees or employee for employment or employing'\n",
    "\n",
    "print(pos_tag(word_tokenize(leaves1)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(leaves2)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(leaves3)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(emps1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization - spaCy\n",
    "\n",
    "- **`lemma_`** property on token applied on a NLP pipeline document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1b81ef8ab88>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No one leaves him like this"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(leaves1)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There are leaves all over the lawn"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(leaves2)\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dried leaves on trees"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3 = nlp(leaves3)\n",
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employers can employ employees or employee for employment or employing"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4 = nlp(emps1)\n",
    "doc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to shows lemmas for other documents\n",
    "\n",
    "def show_lemmas(text):\n",
    "    \"\"\"\n",
    "    Helper function to print lemmas of words(tokens) of a provided text\n",
    "    I/P:\n",
    "        - text(NLP document object): NLP doc object\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'{\"Token Text\":{15}} {\"Token Lemma_\":{15}} {\"Token POS\":{12}} {\"Token Lemma Hash Ref\":{23}} ')\n",
    "    print('-'*65)\n",
    "    \n",
    "    for token in text:\n",
    "        print(f'{token.text:{15}} {token.lemma_:{15}} {token.pos_:{12}} {token.lemma:<{23}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "No              no              DET          13055779130471031426   \n",
      "one             one             NOUN         17454115351911680600   \n",
      "leaves          leave           VERB         9707179535890930240    \n",
      "him             he              PRON         1655312771067108281    \n",
      "like            like            ADP          18194338103975822726   \n",
      "this            this            DET          1995909169258310477    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "There           there           PRON         2112642640949226496    \n",
      "are             be              AUX          10382539506755952630   \n",
      "leaves          leave           NOUN         9707179535890930240    \n",
      "all             all             ADV          13409319323822384369   \n",
      "over            over            ADP          5456543204961066030    \n",
      "the             the             DET          7425985699627899538    \n",
      "lawn            lawn            NOUN         8580092763855978974    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "Dried           dry             VERB         4116088610979501248    \n",
      "leaves          leave           VERB         9707179535890930240    \n",
      "on              on              ADP          5640369432778651323    \n",
      "trees           tree            NOUN         5236966400857015965    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "Employers       employer        NOUN         10831503532707336449   \n",
      "can             can             AUX          6635067063807956629    \n",
      "employ          employ          VERB         12763792191920418315   \n",
      "employees       employee        NOUN         8285577505045524338    \n",
      "or              or              CCONJ        3740602843040177340    \n",
      "employee        employee        NOUN         8285577505045524338    \n",
      "for             for             ADP          16037325823156266367   \n",
      "employment      employment      NOUN         10954873364127974648   \n",
      "or              or              CCONJ        3740602843040177340    \n",
      "employing       employ          VERB         12763792191920418315   \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Employers', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('employ', 'VB'),\n",
       " ('employees', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('employee', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('employment', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('employing', 'VBG')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_op = [('Employers', 'NNS'), ('can', 'MD'), ('employ', 'VB'), ('employees', 'NNS'), ('or', 'CC'), ('employee', 'NN'), ('for', 'IN'), ('employment', 'NN'), ('or', 'CC'), ('employing', 'VBG')]\n",
    "nltk_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with TFIDF\n",
    "\n",
    "- For using NLTK's lemmatization within TFIDF, create a dunder call method within a custom class and call this object by passing the document as a parameter\n",
    "- The **`dunder call method will use an instance of the WordNetLemmatizer`** to convert each token into its lemmatized form\n",
    "- Pass the above to the **`tokenizer parameter of TFIDF instance`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employees can take vacation', 'Employee who employs employees']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = ['Employees can take vacation','Employee who employs employees']\n",
    "doc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employees can take vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employee who employs employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      random_text\n",
       "0     Employees can take vacation\n",
       "1  Employee who employs employees"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_df = pd.DataFrame({'random_text':['Employees can take vacation','Employee who employs employees']})\n",
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(token) for token in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizer at 0x1b85e110688>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tknzr = LemmaTokenizer()\n",
    "lem_tknzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x000001B85E110688>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1 = TfidfVectorizer(tokenizer=lem_tknzr)\n",
    "tfidf_emps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53404633, 0.        , 0.37997836, 0.53404633, 0.53404633,\n",
       "        0.        ],\n",
       "       [0.        , 0.49844628, 0.70929727, 0.        , 0.        ,\n",
       "        0.49844628]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1.fit_transform(doc5).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53404633, 0.        , 0.37997836, 0.53404633, 0.53404633,\n",
       "        0.        ],\n",
       "       [0.        , 0.49844628, 0.70929727, 0.        , 0.        ,\n",
       "        0.49844628]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1.fit_transform(random_df['random_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_emps1_dfmapper = DataFrameMapper(\n",
    "    [\n",
    "        ( 'random_text', tfidf_emps1 )\n",
    "    ], \n",
    "    df_out = True,\n",
    "    input_df = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrameMapper(df_out=True, drop_cols=[],\n",
       "                features=[('random_text',\n",
       "                           TfidfVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x000001B85E110688>))],\n",
       "                input_df=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1_dfmapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text_can</th>\n",
       "      <th>random_text_employ</th>\n",
       "      <th>random_text_employee</th>\n",
       "      <th>random_text_take</th>\n",
       "      <th>random_text_vacation</th>\n",
       "      <th>random_text_who</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498446</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   random_text_can  random_text_employ  random_text_employee  \\\n",
       "0         0.534046            0.000000              0.379978   \n",
       "1         0.000000            0.498446              0.709297   \n",
       "\n",
       "   random_text_take  random_text_vacation  random_text_who  \n",
       "0          0.534046              0.534046         0.000000  \n",
       "1          0.000000              0.000000         0.498446  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1_dfmapper.fit_transform(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below shows that effect of converting to **lower case before lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employees', 'employee', 'can', 'take', 'vacation']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[WordNetLemmatizer().lemmatize(token) for token in word_tokenize('Employees employee can take vacation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['employee', 'employee', 'can', 'take', 'vacation']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[WordNetLemmatizer().lemmatize(token) for token in word_tokenize('employees employee can take vacation')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple illustration of the dunder call method within a custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitnCall:\n",
    "    def __init__(self):\n",
    "        print('Init')\n",
    "    \n",
    "    def __call__(self):\n",
    "        print('Call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n"
     ]
    }
   ],
   "source": [
    "my_inc = InitnCall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call\n"
     ]
    }
   ],
   "source": [
    "my_inc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitCallwPara:\n",
    "    def __init__(self):\n",
    "        print('Init')\n",
    "    def __call__(self, a, b):\n",
    "        print('Call with parameters: ', a ,' and ', b)\n",
    "        print('Sum = ', a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n"
     ]
    }
   ],
   "source": [
    "my_incwp = InitCallwPara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call with parameters:  5  and  6\n",
      "Sum =  11\n"
     ]
    }
   ],
   "source": [
    "my_incwp(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __call__ let's your object act like a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF illustrating tokenization, lemmatization (NLTK), stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employees can take vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employee who employs employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      random_text\n",
       "0     Employees can take vacation\n",
       "1  Employee who employs employees"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(token) for token in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizer at 0x1b85e148c48>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tknzr = LemmaTokenizer()\n",
    "lem_tknzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small example of illustrating if stop words will be removed\n",
    "my_stop_words = ['who']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['who'],\n",
       "                tokenizer=<__main__.LemmaTokenizer object at 0x000001B85E148C48>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls = TfidfVectorizer(analyzer='word', \n",
    "                            tokenizer=lem_tknzr, \n",
    "                            stop_words=my_stop_words\n",
    "                           )\n",
    "tfidf_tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = '.'\n",
    "tfidf_tls_dfm = DataFrameMapper(\n",
    "    [\n",
    "        ( 'random_text', tfidf_tls, {'alias':alias})\n",
    "    ], \n",
    "    df_out = True,\n",
    "    input_df = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>._can</th>\n",
       "      <th>._employ</th>\n",
       "      <th>._employee</th>\n",
       "      <th>._take</th>\n",
       "      <th>._vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ._can  ._employ  ._employee    ._take  ._vacation\n",
       "0  0.534046  0.000000    0.379978  0.534046    0.534046\n",
       "1  0.000000  0.574962    0.818180  0.000000    0.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_df = tfidf_tls_dfm.fit_transform(random_df)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['._can', '._employ', '._employee', '._take', '._vacation']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls_dfm.transformed_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'employ', 'employee', 'take', 'vacation']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the orignial words instead of the one from dfm\n",
    "alias = '.'\n",
    "[x.replace(alias+'_', '') for x in tfidf_tls_dfm.transformed_names_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>._can</th>\n",
       "      <th>._employ</th>\n",
       "      <th>._employee</th>\n",
       "      <th>._take</th>\n",
       "      <th>._vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ._can  ._employ  ._employee    ._take  ._vacation\n",
       "0  0.534046  0.000000    0.379978  0.534046    0.534046\n",
       "1  0.000000  0.574962    0.818180  0.000000    0.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls_dfm.fit_transform(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF illustrating tokenization, lemmatization (spaCy), stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employees can take vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employee who employs employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      random_text\n",
       "0     Employees can take vacation\n",
       "1  Employee who employs employees"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1b81ef8ab88>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizerSpacy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, doc):\n",
    "        return [token.lemma_ for token in nlp(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizerSpacy at 0x1b81fc43888>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tknzr_sp = LemmaTokenizerSpacy()\n",
    "lem_tknzr_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['who'],\n",
       "                tokenizer=<__main__.LemmaTokenizerSpacy object at 0x000001B81FC43888>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls_sp = TfidfVectorizer(analyzer='word', \n",
    "                               tokenizer=lem_tknzr_sp, \n",
    "                               stop_words=my_stop_words\n",
    "                              )\n",
    "tfidf_tls_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = '.'\n",
    "tfidf_tls_dfm_spacy = DataFrameMapper(\n",
    "    [\n",
    "        ( 'random_text', tfidf_tls_sp, {'alias':alias})\n",
    "    ], \n",
    "    df_out = True,\n",
    "    input_df = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>._can</th>\n",
       "      <th>._employ</th>\n",
       "      <th>._employee</th>\n",
       "      <th>._take</th>\n",
       "      <th>._vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ._can  ._employ  ._employee    ._take  ._vacation\n",
       "0  0.534046  0.000000    0.379978  0.534046    0.534046\n",
       "1  0.000000  0.574962    0.818180  0.000000    0.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_df_sp = tfidf_tls_dfm_spacy.fit_transform(random_df)\n",
    "dtm_df_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF parameters: max_df and min_df:\n",
    "- **TFIDF** method for vectorizing words / tokens\n",
    "- **max_df** and **min_df**\n",
    "- **vocabularies generated** with different tfidf settings for max_df and min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_df\n",
    "\n",
    "`max_df` is used for removing terms that appear **too frequently**, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    "- `max_df = 0.50` means \"ignore terms that appear in **more than 50% of the documents**\".\n",
    "- `max_df = 25` means \"ignore terms that appear in **more than 25 documents**\".\n",
    "\n",
    "The default `max_df` is `1.0`, which means \"ignore terms that appear in **more than 100% of the documents**\". Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_df\n",
    "\n",
    "`min_df` is used for removing terms that appear **too infrequently**. For example:\n",
    "\n",
    "- `min_df` = 0.01 means \"ignore terms that appear in **less than 1% of the documents**\".\n",
    "- `min_df` = 5 means \"ignore terms that appear in **less than 5 documents**\".\n",
    "The default `min_df` is `1`, which means \"ignore terms that appear in **less than 1 document**\". Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dtm(doc_list, vectorizer, reqd_output_format='df'):\n",
    "    \"\"\"\n",
    "    Desc: \n",
    "        Creates a document term matrix\n",
    "    Input:\n",
    "        - doc_list: list of documents for which dtm is to be created\n",
    "        - vectorizer: Vectorizer used for numerical representation \n",
    "          of features\n",
    "        - reqd_output_format: specifies if return format is required\n",
    "          as a pandas dataframe or a sparse matrix\n",
    "    Output:\n",
    "        - DTM: Document term matrix in DF or Sparse Matrix\n",
    "    \"\"\"\n",
    "    dtm = vectorizer.fit_transform(doc_list)\n",
    "    \n",
    "    if reqd_output_format == 'spr':\n",
    "        return dtm\n",
    "    # else return as a pandas dataframe\n",
    "    return pd.DataFrame(dtm.toarray(),\n",
    "                       columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = [\"My name is Ashish Sharma\",\n",
    "       \"Ashish likes Python programming language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v1 = TfidfVectorizer(stop_words='english')\n",
    "tfidf_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ashish</th>\n",
       "      <th>language</th>\n",
       "      <th>likes</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>sharma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.579739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335176</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ashish  language     likes  programming    python    sharma\n",
       "0  0.579739  0.000000  0.000000     0.000000  0.000000  0.814802\n",
       "1  0.335176  0.471078  0.471078     0.471078  0.471078  0.000000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_df = 1\n",
    "\n",
    "create_dtm(msg1, tfidf_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ashish': 0,\n",
       " 'sharma': 5,\n",
       " 'likes': 2,\n",
       " 'python': 4,\n",
       " 'programming': 3,\n",
       " 'language': 1}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.9, stop_words='english')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_df = 0.9\n",
    "\n",
    "tfidf_v2 = TfidfVectorizer(stop_words='english',\n",
    "                          max_df=0.9)\n",
    "tfidf_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>likes</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>sharma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  likes  programming  python  sharma\n",
       "0       0.0    0.0          0.0     0.0     1.0\n",
       "1       0.5    0.5          0.5     0.5     0.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dtm(msg1, tfidf_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sharma': 4, 'likes': 1, 'python': 3, 'programming': 2, 'language': 0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v2.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get to know which words get ignored due to max_df settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ashish'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words due to max_df and min_df\n",
    "\n",
    "tfidf_v2.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ashish'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf_v2 filters out words that are present in 90% of the documents\n",
    "# tfidf_v1 does not filter out any word\n",
    "\n",
    "# Let's see the difference b/w these 2 dictionaries to understand which\n",
    "# words are present across 90% of all articles\n",
    "\n",
    "set(tfidf_v1.vocabulary_) - set(tfidf_v2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.5, stop_words='english')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_df = 0.5\n",
    "\n",
    "tfidf_v3 = TfidfVectorizer(stop_words='english',\n",
    "                          max_df=0.5)\n",
    "tfidf_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>likes</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>sharma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  likes  programming  python  sharma\n",
       "0       0.0    0.0          0.0     0.0     1.0\n",
       "1       0.5    0.5          0.5     0.5     0.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dtm(msg1, tfidf_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sharma': 4, 'likes': 1, 'python': 3, 'programming': 2, 'language': 0}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v3.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ashish'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words due to max_df and min_df\n",
    "\n",
    "tfidf_v3.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg1_spr_arr = create_dtm_spr_arr(msg1, tfidf_v1)\n",
    "msg1_spr_arr = create_dtm(msg1, tfidf_v1, 'spr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(msg1_spr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg1_spr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Ashish Sharma\n",
      "Ashish likes Python programming language\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(msg1)):\n",
    "    print(msg1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.579739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335176</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.471078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.579739  0.000000  0.000000  0.000000  0.000000  0.814802\n",
       "1  0.335176  0.471078  0.471078  0.471078  0.471078  0.000000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(msg1_spr_arr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity scores across documents in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim = cosine_similarity(msg1_spr_arr, msg1_spr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.19431434],\n",
       "       [0.19431434, 1.        ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclist = ['Doc1', 'Doc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_df = pd.DataFrame(cossim, index=doclist, columns=doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc1</th>\n",
       "      <th>Doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.194314</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc1      Doc2\n",
       "Doc1  1.000000  0.194314\n",
       "Doc2  0.194314  1.000000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity scores (visual) across documents in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEICAYAAAD8yyfzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAehUlEQVR4nO3deZwdVZ338c+3O4nssqMkIaAEJYCKhuA8qMDIEnZnGJEAKjNCAxJxhHGE52GyjTM6jtujBjEyEQQk4jpRw6ICsgiadgRikGgMS5qAJEDYlyT9mz/qNFYu3Xdpqqtviu87r3q9btU5derc3Lq/PvecU1WKCMzMrBwdw10BM7NXEgddM7MSOeiamZXIQdfMrEQOumZmJXLQNTMrkYNukyQtlnRAycfcSdJTkjoHuf9Tkl6XXl8s6ZMvoy5XSfrgYPevCkkzJF023PWwDVclg66kEyR1p6DzYAoY73g5ZUbEHhFxQ0FVfJGkMZK+J2mVpMclLZJ0cjrm/RGxWUSsG0zZad9lRdQzIg6LiEtSnU+WdHMR5drwkhSSdh3uerySVC7oSjob+CLw78AOwE7ABcAxw1mvOi4FlgPjgG2ADwB/HtYa5SjTlufJYH8BmA2riKjMArwaeAp4b508ryILyivS8kXgVSltW+DHwGrgUeAmoCOl3QsclF7PAK4Evgk8CSwGJuaOsSPwPWAlcA9wVp36PAW8ZYC0nYEARqT1G4BPAr9M+/2ILFBfDjwBLAR2zu0fwK7p9cXAJ9PrrdL7XAk8ll6Pye13A/BvwC3As8CuadspwO7Ac8C6VIfVwD5kfyhG5Mo4Frh9gPd1BPDbVOflwIya9Hek97g6pZ+cew9fBRYATwMHpc/8m+m93Aecn/vMdgV+ATwOrAK+nbYL+ALwcEq7E9hzgLruksp4Evgp8BXgslz60enzX53+j3bPpY0Fvp/q9gjwldz5c1mBn/MbU90eBZYAx+XSLgZmAz9J7+FXwOtT2o3puE+n47yPOt8BLwXFqeGuQKFvBiYDa/Nf/n7yzAJuA7YHtksn9r+mtE8BFwIj0/JOQCntXtYPus8BhwOdab/bUloH8BtgGjAKeB2wDDh0gPr8jCy4HQ/sVJPW35dxKfB6smBzF/AHsuAzgiz4fCO3/0BBdxuyoLgJsDnwHeCHuf1uAO4H9kjljkzbTknpJwM319T1LuCw3PoPgHMGeM8HAHul/6s3kQXs96S0nVJwmJKOuw3pj1J6D48D+6V9N0rv+b/T+9g5/X98KOW/Avh/ubzvSNsPTZ/RlmQBeHfgtQPU9Vbg82R/rN+V6nZZStuNLGAdnOr6z+nzGZXOizvIgvumNcefQeOg29TnnMpeDvx9Snsr2R+YPXL/Z48Ck1L65cC8/s6RRt8BL8Usbfmz8WXYBlgVEWvr5DkRmBURD0fESmAm8P6UtgZ4LTAuItZExE2RzsR+3BwRCyLrb70UeHPavg+wXUTMiogXIutT/TpZUO3Pe8laE/8C3CPpdkn71Kn/NyLiTxHxOHAV8KeI+Fl6z98B9q6zLwAR8UhEfC8inomIJ8latfvXZLs4IhZHxNqIWNOoTOAS4CQASVuTBbZvDXD8GyJiUUT0RsSdZMGx7/gnAj+LiCvSZ/BIRNye2/2/I+KWiOgl+7zeB5wXEU9GxL3A51j/8xwH7BgRz0XEzbntm5O1EBURv4+IB2vrKWknss/zXyLi+Yi4kazV2ed9wE8i4qfp/+izwMbA/yELcjsCH4+Ip2uO34xmP+cjgXsj4hvps/ofsl9Zf5cr6/sR8eu07+XAW+oct5XvgA1C1YLuI8C2kkbUybMj2c/QPvelbQD/SdbCuFbSMknn1innodzrZ4CN0nHHATtKWt23AP+XrH/5JSLisYg4NyL2SHluB34oSQMcN9/f+2w/65vVqTMAkjaR9DVJ90l6guxn5pY1faTLG5VT4zLgKEmbAccBN/UXyNLx95V0vaSVkh4HTif7WQvZT/I/1TlOvl7bkrUqaz/P0en1P5O1ZH+dZp/8A0BEXEfWTTAb+LOkOZK26OdYOwKPRcTTNeXn019cT38IlqfjjwXua9AAqKfZz3kcsG/N+XYi8Jpc/tpztd450sp3wAahakH3VrKf/e+pk2cF2YnaZ6e0jdRaOiciXgccBZwt6d0t1mE5cE9EbJlbNo+IwxvtGBGryFpLOwJbt3jcVpwDvAHYNyK2IPvZDFmAerE6dfZ/SVpEPED2//83ZC3NS+vs/y1gPjA2Il5N9nO279jLyX5WN3PsVfylNdtnJ+CBVKeHIuLUiNgROA24oG+kPiK+FBFvI+tC2Q34eD/HehDYStKmNeX3We9cSn8ox6bjLwd2GqAB8DRZ106f1/STp1nLgV/UnG+bRcQZgymsoO+A1VGpoJt+ik0DZkt6T2rRjZR0mKTPpGxXAOdL2k7Stin/ZQCSjpS0a/ryPEE2WNTqdK1fA09I+oSkjSV1StpzoC4DSf+R0kdI2hw4A1gaEY+0/B/QvM3JWkurU1fA9Bb3/zMwRtKomu3fJGtd7kXWp1vv+I9GxHOSJgEn5NIuBw6SdFz6P9lGUr8/h1PXzpXAv0naXNI44Gz+8nm+V9KYlP0xsoC9TtI+qbU9kiwA9g0M1pZ/H9ANzJQ0Kk07PCqX5UrgCEnvTmWdAzxPNk7wa7Kg/WlJm0raSNJ+ab/bgXeledivBs6r83/VyI+B3SS9P53rI9P7273J/f9MNu4AFPYdsDoqFXQBIuLzZF+888lGjZcDU4EfpiyfJPsi3QksAv4nbQMYTzaw9RRZq+2CaHFubgoER5H1m91D1hq7iGxApD+bkAWo1WQDbuPIRsSH0hfJ+h5XkQ0qXt3i/teRjdg/JGlVbvsPyOr/g5qf5LU+DMyS9CTZH70r+xIi4n6yAcpzyAaAbucv/eX9+QhZ4FwG3EzWip6b0vYBfiXpKbKW9Ucj4h5gC7J+9sfIugceIfuF0Z8TgH1TXaaT/WHpq+sSsn7sL5P9Xx4FHJX68vvOg13JBiV7yPqAiYifAt8mOwd/QxY4ByX1yR9CNmawgqwr4T/IBv6aMQO4JHVNHEcB3wGrr29k3qwQkv4EnBYRPxvuupi1o8q1dG34SDqW7Cf8dcNdF7N25aBrhZB0A9mFC2emUXyzDZ6kuZIelvS7AdIl6UuSlkq6U9JbG5XpoGuFiIgDImL7iLhmuOtiVqCLyS66GshhZP3g44EusoZHXQ66ZmYDSBfEPFonyzHANyNzG9l899fWK7PeRQSFmDlzpkfqzKwp06dPH+iioKbpjLc3H3Mu/NVpZC3UPnMiYk4LhxvN+hfs9KRt/V4YBCUEXYDp01udBmpVNnPmTMDnha2v77woUwqwrQTZWv39kagb9EsJumZmZVHHy24st6KH7CrEPmNIV7gOxH26ZlYpHSM6ml4KMB/4QJrF8Hbg8YHuOdLHLV0zq5QiW7qSriC7Fem2knrIrkocCRARF5Ld2/lwspsEPUN2i826HHTNrFIGvkFf6yJiSoP0AM5spUwHXTOrlJL7dFvmoGtmleKga2ZWIgddM7MSFTQrYcg46JpZpbila2ZWIgddM7MSFTllbCg46JpZpbila2ZWIg+kmZmVyC1dM7MSOeiamZXIQdfMrEQOumZmJXLQNTMrUUenZy+YmZXGLV0zsxI56JqZlaijvXsX/GBKM6uWTqnppRFJkyUtkbRU0rn9pI+T9HNJd0q6QdKYRmU66JpZpYzq7Gh6qUdSJzAbOAyYAEyRNKEm22eBb0bEm4BZwKca1c9B18wqpbOj+aWBScDSiFgWES8A84BjavJMAH6eXl/fT/pLOOiaWaW00r0gqUtSd27pyhU1GlieW+9J2/LuAI5Nr/8G2FzSNvXq54E0M6uUZvpq+0TEHGDOAMn9FRQ16/8EfEXSycCNwAPA2nrHdNA1s0rpLG7KWA8wNrc+BliRzxARK4C/BZC0GXBsRDxer1B3L5hZpXSq+aWBhcB4SbtIGgUcD8zPZ5C0raS+OHoeMLdRoW7pmlmlNJqV0KyIWCtpKnAN0AnMjYjFkmYB3RExHzgA+JSkIOteOLNRuQ66ZlYpBXYvEBELgAU126blXn8X+G4rZTromlmlNNFtMKwcdM2sUops6Q4FB10zq5RWpowNBwddM6sUB10zsxKNavNOXQddM6sU9+mamZXI3QtmZiVq80ekOeiaWbW4pWtmVqKiLgMeKg66ZlYpbR5zHXTNrFrcvWBmVqI2n6broGtm1dLhlq6ZWXnc0jUzK9FID6SZmZWnw5cBm5mVp927F9q8IW5m1poONb80ImmypCWSlko6t5/0nSRdL+m3ku6UdHjD+g3ubZmZtaeingYsqROYDRwGTACmSJpQk+184MqI2JvsacEXNKqfuxfMrFIKnDI2CVgaEcsAJM0DjgHuyuUJYIv0+tXAiob1G2xtJM0Z7L5mZkNlZEfzi6QuSd25pStX1GhgeW69J23LmwGcJKmH7KnBH2lUv7otXUlbD5QEDNh3kSreBXDkkUc2qoOZWWFauQw4IuYAAzUg+ysoatanABdHxOck/RVwqaQ9I6J3oGM26l5YCdxXc/BI69sPtFP+jcycObO2kmZmQ6bAGWM9wNjc+hhe2n3wIWAyQETcKmkjYFvg4YEKbRR0lwHvjoj7axMkLe8nv5nZsCpwythCYLykXYAHyAbKTqjJcz/wbuBiSbsDG5E1VgfUKOh+EdgqFVzrM01U2sysVB0FzcmKiLWSpgLXAJ3A3IhYLGkW0B0R84FzgK9L+hhZL8DJEVH3133doBsRs+ukfbnVN2FmNtRGFti/EBELyAbI8tum5V7fBezXSplN/U2QdKakLXPrW0n6cCsHMjMrQ1HzdIdKsw3xUyNidd9KRDwGnDo0VTIzG7wir0gbCs1eHNEhSX19FelKjVFDVy0zs8GpypMjrgGulHQhWWfx6cDVQ1YrM7NBavObjDUddD8BnAacQTZH91rgoqGqlJnZYLX7XcaaCroR0Svpv4CbyVq6SyJi3ZDWzMxsEIqcvTAUmgq6kg4ALgHuJWvpjpX0wYi4ceiqZmbWuqo8I+1zwCERsQRA0m7AFcDbhqpiZmaDUZWgO7Iv4AJExB8kjRyiOpmZDVpVgm536tO9NK2fCPxmaKpkZjZ4HWrvZzM0G3TPAM4EziLr072RJu6QbmZWtkq0dCPieUmXApdGRN076JiZDacRRd3xZojUrZ0yMyStAu4GlkhaKWlavf3MzIZLRwv/hqd+9f0j2R109omIbSJia2BfYL90KzMzs7bSITW9DEv9GqR/AJgSEff0bUgPaTsppZmZtZV2D7qN+nRHRsSq2o0RsdJTxsysHW3osxdeGGSamdmwaPeBtEZB982Snuhnu8ieBWRm1lY26CljEdFZVkXMzIpQZPeCpMnA/yd7RtpFEfHpmvQvAAem1U2A7SNiS+po9uIIM7MNQgfFtHTTwxpmAweTPY59oaT56bloAETEx3L5PwLs3bh+ZmYVUuDshUnA0ohYFhEvAPOAY+rkn0J2I7D69Wv6nZiZbQA61NH0IqlLUndu6coVNRpYnlvvSdteQtI4YBfgukb1c/eCmVVKK7MXImIOMGeA5P6awjFA3uOB7zbzcAcHXTOrlAIH0nqAsbn1McCKAfIeT3ZTsIYcdM2sUgqcMrYQGC9pF+ABssB6Qm0mSW8AtgJubaZQB10zq5SiZi9ExFpJU8meht4JzI2IxZJmAd0RMT9lnQLMi4iBuh7W46BrZpVS5MUREbEAWFCzbVrN+oxWynTQNbNKGdHR3td0OeiaWaVoA7/hjZnZBmW4bk7eLAddM6sUt3TNzEq0od9P18xsgyJ3L5iZladT7R3W2rt2ZmYtcveCmVmJPJBmZlYiTxkzMyuRW7pmZiVyn66ZWYk6NXK4q1CXg66ZVYrn6ZqZlcjdC2ZmJfJAmplZiTxlzMysRJ0d7R3W2vtPgplZi0RH00vDsqTJkpZIWirp3AHyHCfpLkmLJX2rUZnt/SfBzKxFRQ2kSeoEZgMHkz2OfaGk+RFxVy7PeOA8YL+IeEzS9g3rV0jtzMzaRIEt3UnA0ohYFhEvAPOAY2rynArMjojHACLi4UaFltLSnTlzZhmHsQ2MzwsbCq20dCV1AV25TXMiYk56PRpYnkvrAfatKWK3VM4tZI9pnxERV9c7prsXzKxSWpkylgLsnAGS+3uWe9SsjwDGAwcAY4CbJO0ZEasHOmYpQXf8jIZ9y/YK8scZJwAwffr0Ya6JtZOifvmot7f5zPXjcw8wNrc+BljRT57bImINcI+kJWRBeOHgDmlmtqGJ3uaX+hYC4yXtImkUcDwwvybPD4EDASRtS9bdsKxeoe5eMLNqaRxMmysmYq2kqcA1ZP21cyNisaRZQHdEzE9ph0i6C1gHfDwiHqlXroOumVVLQUEXICIWAAtqtk3LvQ7g7LQ0xUHXzKqllT7dYeCga2bVUmBLdyg46JpZtfSuHe4a1OWga2bV4u4FM7MSuXvBzKxEDrpmZiVy0DUzK0+sW9N03v5urjDUHHTNrFrc0jUzK5GDrplZiRx0zcxK5KBrZlYiXxxhZlYiXwZsZlYidy+YmZXI3QtmZiVyS9fMrERtHnT9YEozq5be3uaXBiRNlrRE0lJJ5/aTfrKklZJuT8spjcp0S9fMqmXtukKKkdQJzAYOJnvU+kJJ8yPirpqs346Iqc2W66BrZtVS3EDaJGBpRCwDkDQPOAaoDbotcfeCmVVLbzS9SOqS1J1bunIljQaW59Z70rZax0q6U9J3JY1tVD23dM2sWlpo6UbEHGDOAMn93fkxatZ/BFwREc9LOh24BPjresd0S9fMqqW4gbQeIN9yHQOsyGeIiEci4vm0+nXgbY0KdUvXzKqloIE0YCEwXtIuwAPA8cAJ+QySXhsRD6bVo4HfNyrUQdfMqqW3tgdgcCJiraSpwDVAJzA3IhZLmgV0R8R84CxJRwNrgUeBkxuV66BrZtVS4GXAEbEAWFCzbVru9XnAea2U6aBrZtXiey+YmZUnovnuBT+Y0szs5XJL18ysRMXNXhgSDrpmVi1u6ZqZlchB18ysRAXN0x0qDrpmVi1u6ZqZlcgDaWZmJXJL18ysRA66ZmYl8kCamVmJ3NI1MytPrHNL18ysPGvc0jUzK024T9fMrETuXjAzK9G69u5e8NOAC/LaQ9/JkXdfzVF/vJYJnzj1JenbvXMik3/zfY5fs5ixxx66XtpbPv1PHL7oRxy+6EfsdNxhZVXZrJKiN5peGpE0WdISSUslnVsn399JCkkTG5XpoFsAdXQwcfY0rj/sFH4y4QjGTTmSLXZ//Xp5nrn/QW47+Tzu+9aP19u+4+H7s9VbJ3DVW97DNfsex+4fP4URm29aZvXNqmVdNL/UIakTmA0cBkwApkia0E++zYGzgF81Uz0H3QJsM+lNPLX0Pp6+p4feNWu4b95PGHPMu9fL8/R9D7B60RKiZg7hqyfsysO/WEisW8e6Z55l9R13s+Pkd5VZfbNKiTW9TS8NTAKWRsSyiHgBmAcc00++fwU+AzzXTP0cdAuw8egdeHr5Qy+uP9PzZzYZvUNT+z52x93seNi76Nx4I161zVbscOC+bDL2NUNVVbPq6+1tepHUJak7t3TlShoNLM+t96RtL5K0NzA2Itb/CVtH3YG01Lw+BRgDXB0Rt+TSzo+ITw6wXxfQBXDkkUcyvtnabKj00sfbNftwvId+egvb7LMXh/xyHs+tfJRVt95OtPldkszaWguzFyJiDjBngOT+nlv5YuGSOoAvACe3ULuGLd2vAfsDjwBfkvT5XNrfDrRTRMyJiIkRMXHixIb9yhu8Z3seYtNc63STMTvw7IqHm95/8b9fyFV7v4frD/kHEDz5x3uHoJZmrwwFDqT1AGNz62OAFbn1zYE9gRsk3Qu8HZjfaDCtUdCdFBEnRMQXgX2BzSR9X9KrGJ6nF7elRxYuYvPxO7PpzmPoGDmScccfwQPzr2tqX3V0MGrrLQHYcq83sOWb3sCD197SYC8zG1BBA2nAQmC8pF0kjQKOB+b3JUbE4xGxbUTsHBE7A7cBR0dEd71CG83THZU7wFqgS9I04Dpgs0Y1fqWIdevonjqLA6+5CHV2smzu93j8rqXsNfMsHu3+HQ/86Dq2nrgX7/rBVxi11RaMPupA9pr5ERbseSQaOYKDb7ocgDVPPMUvT/o4sc7dC2aD1cQAWXPlRKyVNBW4BugE5kbEYkmzgO6ImF+/hP41CrrdkiZHxNW5isyStAL46mAOWFUrrrqRFVfduN62RdO/9OLrR7sX8cOx+79kv97nX+Anexwx5PUze8Uo8OKIiFgALKjZNm2AvAc0U2bdoBsRJw2w/SLgomYOYGZWpna/90JTU8YknSlpy9z6VpI+PHTVMjMbpOL6dIdEs/N0T42I1X0rEfEY8NJrXc3MhltvNL8Mg2ZveNMhSZEmn6b5u6Ma7GNmVrqq3MT8GuBKSReSTQ4+Hbi6/i5mZsNgTXvP/mk26H4COA04g2x+7rV4IM3M2lC7D6Q1FXQjolfSfwE3k7V0l0REe/85MbNXpip0L0g6ALgEuJespTtW0gcj4sZ6+5mZla0SLV3gc8AhEbEEQNJuwBXA24aqYmZmg1GVgbSRfQEXICL+IGnkENXJzGzQqtLS7U59upem9ROB3wxNlczMBm9dRR7BfgZwJtkjKQTcCFwwVJUyMxus2qeztJtmZy88L+lS4NKIWDnEdTIzG7R279OtexmwMjMkrQLuBpZIWplu72hm1naKfBrwUGh074V/BPYD9omIbSJia7Kbme8n6WNDXjszsxbFumh6GQ6Nuhc+ABwcEav6NkTEMkknkV2V9oWhrJyZWavWrd2w+3RH5gNun4hY6SljZtaO2r1Pt1HQfWGQaWZmw6Ld5+k26tN9s6Qn+lmeBPYqo4JmZq0ociBN0mRJSyQtlXRuP+mnS1ok6XZJN0ua0KjMRo/r6WxYKzOzNlJU90K6b/hs4GCyx7EvlDQ/Iu7KZftWRFyY8h8NfB6YXK/cZi+OMDPbIBR4ccQkYGlELAOQNA84Bngx6EbEE7n8m5LdhbEuB10zq5TeFi4DltQFdOU2zYmIOen1aGB5Lq2HbMpsbRlnAmeTPU3nrxsd00HXzCqlt4WBtBRg5wyQrP526aeM2cBsSScA5wMfrHdMB10zq5QCp4z1AGNz62OAFXXyzwO+2qjQZp8GbGa2QShw9sJCYLykXSSNAo4H5uczSBqfWz0C+GOjQt3SNbNKKaqlGxFrJU0lezBvJzA3IhZLmgV0R8R8YKqkg4A1wGM06FoAB10zq5giL46IiAXAgppt03KvP9pqmQ66ZlYprcxeGA4OumZWKa3MXhgODrpmVilt/uAIB10zqxYHXTOzEjnompmVaO264a5BfQ66ZlYpbumamZXIQdfMrEQOumZmJXLQNTMrkYOumVmJ1q4d7hrU56BrZpUS4cuAzcxK4+4FM7MSOeiamZXIQdfMrEQOumZmJWr32Qt+MKWZVUpvb/NLI5ImS1oiaamkc/tJP1vSXZLulPRzSeMalemga2aV0hvNL/VI6gRmA4cBE4ApkibUZPstMDEi3gR8F/hMo/o56JpZpRTY0p0ELI2IZRHxAjAPOCafISKuj4hn0uptwJhGhTromlmltBJ0JXVJ6s4tXbmiRgPLc+s9adtAPgRc1ah+pQyk/XHGCWUcxjYwM2fOHO4qWAW1MpAWEXOAOQMkq79d+s0onQRMBPZvdEy1+yVzVSKpK33IZi/yedGeJP0VMCMiDk3r5wFExKdq8h0EfBnYPyIeblSuuxfK1dU4i70C+bxoTwuB8ZJ2kTQKOB6Yn88gaW/ga8DRzQRccNA1M+tXRKwFpgLXAL8HroyIxZJmSTo6ZftPYDPgO5JulzR/gOJe5O6FEknqjoiJw10Pay8+L15Z3NItl/vtrD8+L15B3NI1MyuRW7pmZiVy0DUzK5GD7ssgaV0asVws6Y5084tB/Z9KmppuqhGSti26rlaegs+Ly9MNV34naa6kkUXX18rloPvyPBsRb4mIPYCDgcOB6YMs6xbgIOC+oipnw6bI8+Jy4I3AXsDGwCnFVNGGi4NuQdLE6C5gqjIbSfqGpEWSfivpQMjuXCTps2n7nZI+kvb/bUTcO4xvwYZAAefFgkiAX9PEDVWsvfkm5gWKiGXpZ+T2wElp216S3ghcK2k34O+BXYC9I2KtpK2Hr8ZWhiLOi9St8H7go+XW3ormlm7x+m6S8Q7gUoCIuJus22A3si6EC9PVLkTEo8NRSSvdyz0vLgBujIibyqmuDRW3dAsk6XXAOuBh+r9DEWm7J0e/grzc80LSdGA74LQhqaCVyi3dgkjaDrgQ+Erqf7sRODGl7QbsBCwBrgVOlzQipbl7ocJe7nkh6RTgUGBKRLT5IxetGQ66L8/GfVODgJ+RfXH6bhJ7AdApaRHwbeDkiHgeuAi4H7hT0h3ACQCSzpLUQzZQcqeki0p+L1acws4LsoC9A3BrKnNamW/EiufLgM3MSuSWrplZiRx0zcxK5KBrZlYiB10zsxI56JqZlchB18ysRA66ZmYl+l+nYG9LBd85FAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "mask = np.zeros_like(cossim_df)\n",
    "mask[np.triu_indices_from(mask)] = 1\n",
    "sns.heatmap(data=cossim_df,\n",
    "            mask=mask,\n",
    "            linewidth=0.7,\n",
    "            cmap='RdYlGn',\n",
    "            annot=True,\n",
    "            linecolor='grey')\n",
    "plt.title('Cosine Similarity across documents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

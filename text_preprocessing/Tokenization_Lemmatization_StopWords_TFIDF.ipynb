{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline covers: \n",
    "\n",
    "- Tokenization.\n",
    "- Removing the stop words (including additional stop words).\n",
    "- Lemmatization \n",
    "- Generate TF-IDF numerical vectors (including bi-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex\n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "\n",
    "# spaCy visualization\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Random Text for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is not the best text, but it is better than good. \n",
      "It includes some words including good. \n",
      "I could include more but let's not. \n",
      "Live and let live.  \n",
      "The cat jumped over the other cats.Not just another cat.\n",
      "June, the girl, was born in June, the month.\n",
      "Some of the worst months are worse than some of the bad years.\n",
      "Happy New Year, Ashish.\n",
      "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
      "The legislation of the legislature is a weird set of words.\n",
      "\n",
      "------------------------------\n",
      "# of words in text :  87\n",
      "------------------------------\n",
      "# of unique words  :  65\n",
      "------------------------------\n",
      "# of tokens        :  87\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "This is not the best text, but it is better than good. \n",
    "It includes some words including good. \n",
    "I could include more but let's not. \n",
    "Live and let live.  \n",
    "The cat jumped over the other cats.Not just another cat.\n",
    "June, the girl, was born in June, the month.\n",
    "Some of the worst months are worse than some of the bad years.\n",
    "Happy New Year, Ashish.\n",
    "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
    "The legislation of the legislature is a weird set of words.\n",
    "\"\"\"\n",
    "print(text1)\n",
    "\n",
    "list_of_text = (text1.strip()).split(\" \")\n",
    "\n",
    "print('-'*30)\n",
    "print('# of words in text : ', len((text1.strip()).split(\" \")))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of unique words  : ', len(set(list_of_text)))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of tokens        : ', len((text1.strip()).split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all words from a text using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_txt1 = \"Hi New York! What's new and happening and rocking your world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_list_of_words = re.findall(r'\\w+', tmp_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(regex_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " 'What',\n",
       " 's',\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) missing if we simply only extract words\n",
    "- What's --> What and s\n",
    "- Duplicate words included since we are only using a simple Regex command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all words using NLTK word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words = word_tokenize(tmp_txt1)\n",
    "print(type(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " '!',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world',\n",
       " '?']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words\n",
    "# print(nltk_word_tkn_list_of_words, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) included\n",
    "- What's --> What and 's\n",
    "- Duplicated words included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming - NLTK\n",
    "\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer (English Stemmer, Porter2 Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Porter Stemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "p_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Porter)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fairli         \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Porter)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {p_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x15ae04e6c48>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Snowball Stemmer\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "s_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fair           \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {s_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "generous     ---> generous       \n",
      "generously   ---> generous       \n",
      "generation   ---> generat        \n",
      "generate     ---> generat        \n"
     ]
    }
   ],
   "source": [
    "words2 = ['generous', 'generously', 'generation', 'generate']\n",
    "\n",
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word2 in words2:\n",
    "    print(f'{word2:{12}} ---> {s_stemmer.stem(word2):{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all words using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO @@@@@@@@@@@@@@@@@@@@@\n",
    "1. a text with emails, url etc, and see what tokens come from nltk or spacy (spacy should keep emails, urls, U.S., etc. intact. \n",
    "    - Another text \"A 5km cab ride in NYC costs $10.43\"\n",
    "    - \"Let's visit St. Louis in the U.S. next summer\"\n",
    "2. Language library small and large have limited words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

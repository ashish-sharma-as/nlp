{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module covers: \n",
    "\n",
    "- Tokenization.\n",
    "- Stemming \n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sharmaa1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Regex\n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# POS for nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# spaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Random Text for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is not the best text, but it is better than good. \n",
      "It includes some words including good. \n",
      "I could include more but let's not. \n",
      "Live and let live.  \n",
      "The cat jumped over the other cats.Not just another cat.\n",
      "June, the girl, was born in June, the month.\n",
      "Some of the worst months are worse than some of the bad years.\n",
      "Happy New Year, Ashish.\n",
      "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
      "The legislation of the legislature is a weird set of words.\n",
      "\n",
      "------------------------------\n",
      "# of words in text :  87\n",
      "------------------------------\n",
      "# of unique words  :  65\n",
      "------------------------------\n",
      "# of tokens        :  87\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "This is not the best text, but it is better than good. \n",
    "It includes some words including good. \n",
    "I could include more but let's not. \n",
    "Live and let live.  \n",
    "The cat jumped over the other cats.Not just another cat.\n",
    "June, the girl, was born in June, the month.\n",
    "Some of the worst months are worse than some of the bad years.\n",
    "Happy New Year, Ashish.\n",
    "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
    "The legislation of the legislature is a weird set of words.\n",
    "\"\"\"\n",
    "print(text1)\n",
    "\n",
    "list_of_text = (text1.strip()).split(\" \")\n",
    "\n",
    "print('-'*30)\n",
    "print('# of words in text : ', len((text1.strip()).split(\" \")))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of unique words  : ', len(set(list_of_text)))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of tokens        : ', len((text1.strip()).split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all words from a text using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_txt1 = \"Hi New York! What's new and happening and rocking your world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_list_of_words = re.findall(r'\\w+', tmp_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(regex_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " 'What',\n",
       " 's',\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) missing if we simply only extract words\n",
    "- What's --> What and s\n",
    "- Duplicate words included since we are only using a simple Regex command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization - NLTK\n",
    "\n",
    "- Get all words using NLTK word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words = word_tokenize(tmp_txt1)\n",
    "print(type(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " '!',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words\n",
    "# print(nltk_word_tkn_list_of_words, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) included\n",
    "- What's --> What and 's\n",
    "- Duplicated words included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming - NLTK\n",
    "\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer (English Stemmer, Porter2 Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Porter Stemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "p_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Porter)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fairli         \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Porter)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {p_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x1ef89f55988>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Snowball Stemmer\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "s_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fair           \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {s_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "generous     ---> generous       \n",
      "generously   ---> generous       \n",
      "generation   ---> generat        \n",
      "generate     ---> generat        \n"
     ]
    }
   ],
   "source": [
    "words2 = ['generous', 'generously', 'generation', 'generate']\n",
    "\n",
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word2 in words2:\n",
    "    print(f'{word2:{12}} ---> {s_stemmer.stem(word2):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "employ       ---> employ         \n",
      "employs      ---> employ         \n",
      "employing    ---> employ         \n",
      "employment   ---> employ         \n",
      "employee     ---> employe        \n",
      "employees    ---> employe        \n",
      "employer     ---> employ         \n",
      "employers    ---> employ         \n"
     ]
    }
   ],
   "source": [
    "words3 = ['employ', 'employs', 'employing', 'employment', 'employee', 'employees', 'employer', 'employers']\n",
    "\n",
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word3 in words3:\n",
    "    print(f'{word3:{12}} ---> {s_stemmer.stem(word3):{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization - NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General process for using this is to first tokenize your text (usually token will be word)\n",
    "- Subsequently, use an instance of the WordNetLemmatizer and call the lemmatize method on each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easily         \n",
      "fairly       ---> fairly         \n",
      "fairness     ---> fairness       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {lem.lemmatize(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "generous     ---> generous       \n",
      "generously   ---> generously     \n",
      "generation   ---> generation     \n",
      "generate     ---> generate       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word2 in words2:\n",
    "    print(f'{word2:{12}} ---> {lem.lemmatize(word2):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "employ       ---> employ         \n",
      "employs      ---> employ         \n",
      "employing    ---> employing      \n",
      "employment   ---> employment     \n",
      "employee     ---> employee       \n",
      "employees    ---> employee       \n",
      "employer     ---> employer       \n",
      "employers    ---> employer       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word3 in words3:\n",
    "    print(f'{word3:{12}} ---> {lem.lemmatize(word3):{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization dependent on POS of word**\n",
    "- Below shows an example of why it is important to provide a POS tag of the word that will be lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd = 'content'\n",
    "wrd = 'strips'\n",
    "wrd = 'lead'\n",
    "wrd = 'leaves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd, pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leave'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('leaves', 'NNS')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)\n",
    "pos_tag(['leaves'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('No', 'DT'), ('one', 'NN'), ('leaves', 'VBZ'), ('him', 'PRP'), ('like', 'IN'), ('this', 'DT')]\n",
      "------------------------------\n",
      "[('There', 'EX'), ('are', 'VBP'), ('leaves', 'VBZ'), ('all', 'DT'), ('over', 'IN'), ('the', 'DT'), ('lawn', 'NN')]\n",
      "------------------------------\n",
      "[('Dried', 'NNP'), ('leaves', 'VBZ'), ('on', 'IN'), ('trees', 'NNS')]\n",
      "------------------------------\n",
      "[('Employers', 'NNS'), ('can', 'MD'), ('employ', 'VB'), ('employees', 'NNS'), ('or', 'CC'), ('employee', 'NN'), ('for', 'IN'), ('employment', 'NN'), ('or', 'CC'), ('employing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "leaves1 = 'No one leaves him like this'\n",
    "leaves2 = 'There are leaves all over the lawn'\n",
    "leaves3 = 'Dried leaves on trees'\n",
    "emps1 = 'Employers can employ employees or employee for employment or employing'\n",
    "\n",
    "print(pos_tag(word_tokenize(leaves1)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(leaves2)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(leaves3)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(emps1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization - spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1efd2e07188>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No one leaves him like this"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(leaves1)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There are leaves all over the lawn"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(leaves2)\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dried leaves on trees"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3 = nlp(leaves3)\n",
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employers can employ employees or employee for employment or employing"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4 = nlp(emps1)\n",
    "doc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to shows lemmas for other documents\n",
    "\n",
    "def show_lemmas(text):\n",
    "    \"\"\"\n",
    "    Helper function to print lemmas of words(tokens) of a provided text\n",
    "    I/P:\n",
    "        - text(NLP document object): NLP doc object\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'{\"Token Text\":{15}} {\"Token Lemma_\":{15}} {\"Token POS\":{12}} {\"Token Lemma Hash Ref\":{23}} ')\n",
    "    print('-'*65)\n",
    "    \n",
    "    for token in text:\n",
    "        print(f'{token.text:{15}} {token.lemma_:{15}} {token.pos_:{12}} {token.lemma:<{23}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "No              no              DET          13055779130471031426   \n",
      "one             one             NOUN         17454115351911680600   \n",
      "leaves          leave           VERB         9707179535890930240    \n",
      "him             he              PRON         1655312771067108281    \n",
      "like            like            ADP          18194338103975822726   \n",
      "this            this            DET          1995909169258310477    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "There           there           PRON         2112642640949226496    \n",
      "are             be              AUX          10382539506755952630   \n",
      "leaves          leave           NOUN         9707179535890930240    \n",
      "all             all             ADV          13409319323822384369   \n",
      "over            over            ADP          5456543204961066030    \n",
      "the             the             DET          7425985699627899538    \n",
      "lawn            lawn            NOUN         8580092763855978974    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "Dried           dry             VERB         4116088610979501248    \n",
      "leaves          leave           VERB         9707179535890930240    \n",
      "on              on              ADP          5640369432778651323    \n",
      "trees           tree            NOUN         5236966400857015965    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "Employers       employer        NOUN         10831503532707336449   \n",
      "can             can             AUX          6635067063807956629    \n",
      "employ          employ          VERB         12763792191920418315   \n",
      "employees       employee        NOUN         8285577505045524338    \n",
      "or              or              CCONJ        3740602843040177340    \n",
      "employee        employee        NOUN         8285577505045524338    \n",
      "for             for             ADP          16037325823156266367   \n",
      "employment      employment      NOUN         10954873364127974648   \n",
      "or              or              CCONJ        3740602843040177340    \n",
      "employing       employ          VERB         12763792191920418315   \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Employers', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('employ', 'VB'),\n",
       " ('employees', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('employee', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('employment', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('employing', 'VBG')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_op = [('Employers', 'NNS'), ('can', 'MD'), ('employ', 'VB'), ('employees', 'NNS'), ('or', 'CC'), ('employee', 'NN'), ('for', 'IN'), ('employment', 'NN'), ('or', 'CC'), ('employing', 'VBG')]\n",
    "nltk_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

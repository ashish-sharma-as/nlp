{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module covers: \n",
    "\n",
    "- Tokenization.\n",
    "- Stemming \n",
    "- Lemmatization\n",
    "    - Lemmatization - NLTK\n",
    "    - Lemmatization - spaCy\n",
    "    - Lemmatization - TFIDF tokenization parameter\n",
    "    - Simple illustration of the dunder call method within a custom class\n",
    "- TFIDF (includes tokenization, lemmatization and stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sharmaa1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Regex\n",
    "import re\n",
    "\n",
    "# Standard modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# POS for nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "\n",
    "# DataFrameMapper\n",
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Random Text for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is not the best text, but it is better than good. \n",
      "It includes some words including good. \n",
      "I could include more but let's not. \n",
      "Live and let live.  \n",
      "The cat jumped over the other cats.Not just another cat.\n",
      "June, the girl, was born in June, the month.\n",
      "Some of the worst months are worse than some of the bad years.\n",
      "Happy New Year, Ashish.\n",
      "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
      "The legislation of the legislature is a weird set of words.\n",
      "\n",
      "------------------------------\n",
      "# of words in text :  87\n",
      "------------------------------\n",
      "# of unique words  :  65\n",
      "------------------------------\n",
      "# of tokens        :  87\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "This is not the best text, but it is better than good. \n",
    "It includes some words including good. \n",
    "I could include more but let's not. \n",
    "Live and let live.  \n",
    "The cat jumped over the other cats.Not just another cat.\n",
    "June, the girl, was born in June, the month.\n",
    "Some of the worst months are worse than some of the bad years.\n",
    "Happy New Year, Ashish.\n",
    "Employees who employ another emplopyee are often employed in New York. This is not new.\n",
    "The legislation of the legislature is a weird set of words.\n",
    "\"\"\"\n",
    "print(text1)\n",
    "\n",
    "list_of_text = (text1.strip()).split(\" \")\n",
    "\n",
    "print('-'*30)\n",
    "print('# of words in text : ', len((text1.strip()).split(\" \")))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of unique words  : ', len(set(list_of_text)))\n",
    "\n",
    "print('-'*30)\n",
    "print('# of tokens        : ', len((text1.strip()).split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all words from a text using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_txt1 = \"Hi New York! What's new and happening and rocking your world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_list_of_words = re.findall(r'\\w+', tmp_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(regex_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " 'What',\n",
       " 's',\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) missing if we simply only extract words\n",
    "- What's --> What and s\n",
    "- Duplicate words included since we are only using a simple Regex command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization - NLTK\n",
    "\n",
    "- Get all words using NLTK word tokens using nltk.tokenize\n",
    "- **`word_tokenize()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words = word_tokenize(tmp_txt1)\n",
    "print(type(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(nltk_word_tkn_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'New',\n",
       " 'York',\n",
       " '!',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'and',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'rocking',\n",
       " 'your',\n",
       " 'world',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_word_tkn_list_of_words\n",
    "# print(nltk_word_tkn_list_of_words, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights:`**\n",
    "- Punctuations (Exclamation, Apostrophe and question mark) included\n",
    "- What's --> What and 's\n",
    "- Duplicated words included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming - NLTK\n",
    "\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer (English Stemmer, Porter2 Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Porter Stemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "p_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Porter)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fairli         \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Porter)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {p_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x2095427f408>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Snowball Stemmer\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "s_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easili         \n",
      "fairly       ---> fair           \n",
      "fairness     ---> fair           \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {s_stemmer.stem(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "generous     ---> generous       \n",
      "generously   ---> generous       \n",
      "generation   ---> generat        \n",
      "generate     ---> generat        \n"
     ]
    }
   ],
   "source": [
    "words2 = ['generous', 'generously', 'generation', 'generate']\n",
    "\n",
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word2 in words2:\n",
    "    print(f'{word2:{12}} ---> {s_stemmer.stem(word2):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              STEMMED WORD (Snowball)\n",
      "employ       ---> employ         \n",
      "employs      ---> employ         \n",
      "employing    ---> employ         \n",
      "employment   ---> employ         \n",
      "employee     ---> employe        \n",
      "employees    ---> employe        \n",
      "employer     ---> employ         \n",
      "employers    ---> employ         \n"
     ]
    }
   ],
   "source": [
    "words3 = ['employ', 'employs', 'employing', 'employment', 'employee', 'employees', 'employer', 'employers']\n",
    "\n",
    "print(f'{\"WORD\":{17}} {\"STEMMED WORD (Snowball)\":{15}}')\n",
    "\n",
    "for word3 in words3:\n",
    "    print(f'{word3:{12}} ---> {s_stemmer.stem(word3):{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization - NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General process for using this is to first tokenize your text (usually token will be word)\n",
    "- Subsequently, use an instance of the **`WordNetLemmatizer()`** and call the lemmatize method on each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "run          ---> run            \n",
      "runner       ---> runner         \n",
      "ran          ---> ran            \n",
      "runs         ---> run            \n",
      "easily       ---> easily         \n",
      "fairly       ---> fairly         \n",
      "fairness     ---> fairness       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word:{12}} ---> {lem.lemmatize(word):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "generous     ---> generous       \n",
      "generously   ---> generously     \n",
      "generation   ---> generation     \n",
      "generate     ---> generate       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word2 in words2:\n",
    "    print(f'{word2:{12}} ---> {lem.lemmatize(word2):{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD              LEMMATIZED WORD (WordNetLemmatizer)\n",
      "employ       ---> employ         \n",
      "employs      ---> employ         \n",
      "employing    ---> employing      \n",
      "employment   ---> employment     \n",
      "employee     ---> employee       \n",
      "employees    ---> employee       \n",
      "employer     ---> employer       \n",
      "employers    ---> employer       \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"WORD\":{17}} {\"LEMMATIZED WORD (WordNetLemmatizer)\":{15}}')\n",
    "\n",
    "for word3 in words3:\n",
    "    print(f'{word3:{12}} ---> {lem.lemmatize(word3):{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization dependent on POS of word**\n",
    "- Below shows an example of why it is important to provide a POS tag of the word that will be lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd = 'content'\n",
    "wrd = 'strips'\n",
    "wrd = 'lead'\n",
    "wrd = 'leaves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd, pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leave'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(wrd, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('leaves', 'NNS')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)\n",
    "pos_tag(['leaves'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('No', 'DT'), ('one', 'NN'), ('leaves', 'VBZ'), ('him', 'PRP'), ('like', 'IN'), ('this', 'DT')]\n",
      "------------------------------\n",
      "[('There', 'EX'), ('are', 'VBP'), ('leaves', 'VBZ'), ('all', 'DT'), ('over', 'IN'), ('the', 'DT'), ('lawn', 'NN')]\n",
      "------------------------------\n",
      "[('Dried', 'NNP'), ('leaves', 'VBZ'), ('on', 'IN'), ('trees', 'NNS')]\n",
      "------------------------------\n",
      "[('Employers', 'NNS'), ('can', 'MD'), ('employ', 'VB'), ('employees', 'NNS'), ('or', 'CC'), ('employee', 'NN'), ('for', 'IN'), ('employment', 'NN'), ('or', 'CC'), ('employing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "leaves1 = 'No one leaves him like this'\n",
    "leaves2 = 'There are leaves all over the lawn'\n",
    "leaves3 = 'Dried leaves on trees'\n",
    "emps1 = 'Employers can employ employees or employee for employment or employing'\n",
    "\n",
    "print(pos_tag(word_tokenize(leaves1)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(leaves2)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(leaves3)))\n",
    "print('-'*30)\n",
    "print(pos_tag(word_tokenize(emps1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization - spaCy\n",
    "\n",
    "- **`lemma_`** property on token applied on a NLP pipeline document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2095b49e1c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No one leaves him like this"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(leaves1)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There are leaves all over the lawn"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(leaves2)\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dried leaves on trees"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3 = nlp(leaves3)\n",
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employers can employ employees or employee for employment or employing"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4 = nlp(emps1)\n",
    "doc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to shows lemmas for other documents\n",
    "\n",
    "def show_lemmas(text):\n",
    "    \"\"\"\n",
    "    Helper function to print lemmas of words(tokens) of a provided text\n",
    "    I/P:\n",
    "        - text(NLP document object): NLP doc object\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'{\"Token Text\":{15}} {\"Token Lemma_\":{15}} {\"Token POS\":{12}} {\"Token Lemma Hash Ref\":{23}} ')\n",
    "    print('-'*65)\n",
    "    \n",
    "    for token in text:\n",
    "        print(f'{token.text:{15}} {token.lemma_:{15}} {token.pos_:{12}} {token.lemma:<{23}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "No              no              DET          13055779130471031426   \n",
      "one             one             NOUN         17454115351911680600   \n",
      "leaves          leave           VERB         9707179535890930240    \n",
      "him             he              PRON         1655312771067108281    \n",
      "like            like            ADP          18194338103975822726   \n",
      "this            this            DET          1995909169258310477    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "There           there           PRON         2112642640949226496    \n",
      "are             be              AUX          10382539506755952630   \n",
      "leaves          leave           NOUN         9707179535890930240    \n",
      "all             all             ADV          13409319323822384369   \n",
      "over            over            ADP          5456543204961066030    \n",
      "the             the             DET          7425985699627899538    \n",
      "lawn            lawn            NOUN         8580092763855978974    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "Dried           dry             VERB         4116088610979501248    \n",
      "leaves          leave           VERB         9707179535890930240    \n",
      "on              on              ADP          5640369432778651323    \n",
      "trees           tree            NOUN         5236966400857015965    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "Employers       employer        NOUN         10831503532707336449   \n",
      "can             can             AUX          6635067063807956629    \n",
      "employ          employ          VERB         12763792191920418315   \n",
      "employees       employee        NOUN         8285577505045524338    \n",
      "or              or              CCONJ        3740602843040177340    \n",
      "employee        employee        NOUN         8285577505045524338    \n",
      "for             for             ADP          16037325823156266367   \n",
      "employment      employment      NOUN         10954873364127974648   \n",
      "or              or              CCONJ        3740602843040177340    \n",
      "employing       employ          VERB         12763792191920418315   \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Employers', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('employ', 'VB'),\n",
       " ('employees', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('employee', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('employment', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('employing', 'VBG')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_op = [('Employers', 'NNS'), ('can', 'MD'), ('employ', 'VB'), ('employees', 'NNS'), ('or', 'CC'), ('employee', 'NN'), ('for', 'IN'), ('employment', 'NN'), ('or', 'CC'), ('employing', 'VBG')]\n",
    "nltk_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with TFIDF\n",
    "\n",
    "- For using NLTK's lemmatization within TFIDF, create a dunder call method within a custom class and call this object by passing the document as a parameter\n",
    "- The **`dunder call method will use an instance of the WordNetLemmatizer`** to convert each token into its lemmatized form\n",
    "- Pass the above to the **`tokenizer parameter of TFIDF instance`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employees can take vacation', 'Employee who employs employees']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = ['Employees can take vacation','Employee who employs employees']\n",
    "doc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employees can take vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employee who employs employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      random_text\n",
       "0     Employees can take vacation\n",
       "1  Employee who employs employees"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_df = pd.DataFrame({'random_text':['Employees can take vacation','Employee who employs employees']})\n",
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(token) for token in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizer at 0x209689adfc8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tknzr = LemmaTokenizer()\n",
    "lem_tknzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x00000209689ADFC8>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1 = TfidfVectorizer(tokenizer=lem_tknzr)\n",
    "tfidf_emps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53404633, 0.        , 0.37997836, 0.53404633, 0.53404633,\n",
       "        0.        ],\n",
       "       [0.        , 0.49844628, 0.70929727, 0.        , 0.        ,\n",
       "        0.49844628]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1.fit_transform(doc5).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53404633, 0.        , 0.37997836, 0.53404633, 0.53404633,\n",
       "        0.        ],\n",
       "       [0.        , 0.49844628, 0.70929727, 0.        , 0.        ,\n",
       "        0.49844628]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1.fit_transform(random_df['random_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_emps1_dfmapper = DataFrameMapper(\n",
    "    [\n",
    "        ( 'random_text', tfidf_emps1 )\n",
    "    ], \n",
    "    df_out = True,\n",
    "    input_df = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrameMapper(df_out=True, drop_cols=[],\n",
       "                features=[('random_text',\n",
       "                           TfidfVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x00000209689ADFC8>))],\n",
       "                input_df=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1_dfmapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text_can</th>\n",
       "      <th>random_text_employ</th>\n",
       "      <th>random_text_employee</th>\n",
       "      <th>random_text_take</th>\n",
       "      <th>random_text_vacation</th>\n",
       "      <th>random_text_who</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498446</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   random_text_can  random_text_employ  random_text_employee  \\\n",
       "0         0.534046            0.000000              0.379978   \n",
       "1         0.000000            0.498446              0.709297   \n",
       "\n",
       "   random_text_take  random_text_vacation  random_text_who  \n",
       "0          0.534046              0.534046         0.000000  \n",
       "1          0.000000              0.000000         0.498446  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_emps1_dfmapper.fit_transform(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below shows that effect of converting to **lower case before lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employees', 'employee', 'can', 'take', 'vacation']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[WordNetLemmatizer().lemmatize(token) for token in word_tokenize('Employees employee can take vacation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['employee', 'employee', 'can', 'take', 'vacation']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[WordNetLemmatizer().lemmatize(token) for token in word_tokenize('employees employee can take vacation')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple illustration of the dunder call method within a custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitnCall:\n",
    "    def __init__(self):\n",
    "        print('Init')\n",
    "    \n",
    "    def __call__(self):\n",
    "        print('Call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n"
     ]
    }
   ],
   "source": [
    "my_inc = InitnCall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call\n"
     ]
    }
   ],
   "source": [
    "my_inc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitCallwPara:\n",
    "    def __init__(self):\n",
    "        print('Init')\n",
    "    def __call__(self, a, b):\n",
    "        print('Call with parameters: ', a ,' and ', b)\n",
    "        print('Sum = ', a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n"
     ]
    }
   ],
   "source": [
    "my_incwp = InitCallwPara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call with parameters:  5  and  6\n",
      "Sum =  11\n"
     ]
    }
   ],
   "source": [
    "my_incwp(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __call__ let's your object act like a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF illustrating tokenization, lemmatization (NLTK), stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employees can take vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employee who employs employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      random_text\n",
       "0     Employees can take vacation\n",
       "1  Employee who employs employees"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(token) for token in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizer at 0x209689ef388>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tknzr = LemmaTokenizer()\n",
    "lem_tknzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small example of illustrating if stop words will be removed\n",
    "my_stop_words = ['who']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['who'],\n",
       "                tokenizer=<__main__.LemmaTokenizer object at 0x00000209689EF388>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls = TfidfVectorizer(analyzer='word', \n",
    "                            tokenizer=lem_tknzr, \n",
    "                            stop_words=my_stop_words\n",
    "                           )\n",
    "tfidf_tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = '.'\n",
    "tfidf_tls_dfm = DataFrameMapper(\n",
    "    [\n",
    "        ( 'random_text', tfidf_tls, {'alias':alias})\n",
    "    ], \n",
    "    df_out = True,\n",
    "    input_df = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>._can</th>\n",
       "      <th>._employ</th>\n",
       "      <th>._employee</th>\n",
       "      <th>._take</th>\n",
       "      <th>._vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ._can  ._employ  ._employee    ._take  ._vacation\n",
       "0  0.534046  0.000000    0.379978  0.534046    0.534046\n",
       "1  0.000000  0.574962    0.818180  0.000000    0.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_df = tfidf_tls_dfm.fit_transform(random_df)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['._can', '._employ', '._employee', '._take', '._vacation']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls_dfm.transformed_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'employ', 'employee', 'take', 'vacation']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the orignial words instead of the one from dfm\n",
    "alias = '.'\n",
    "[x.replace(alias+'_', '') for x in tfidf_tls_dfm.transformed_names_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>._can</th>\n",
       "      <th>._employ</th>\n",
       "      <th>._employee</th>\n",
       "      <th>._take</th>\n",
       "      <th>._vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ._can  ._employ  ._employee    ._take  ._vacation\n",
       "0  0.534046  0.000000    0.379978  0.534046    0.534046\n",
       "1  0.000000  0.574962    0.818180  0.000000    0.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls_dfm.fit_transform(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF illustrating tokenization, lemmatization (spaCy), stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employees can take vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employee who employs employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      random_text\n",
       "0     Employees can take vacation\n",
       "1  Employee who employs employees"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2095b49e1c8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizerSpacy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, doc):\n",
    "        return [token.lemma_ for token in nlp(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LemmaTokenizerSpacy at 0x2096d045f08>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tknzr_sp = LemmaTokenizerSpacy()\n",
    "lem_tknzr_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['who'],\n",
       "                tokenizer=<__main__.LemmaTokenizerSpacy object at 0x000002096D045F08>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tls_sp = TfidfVectorizer(analyzer='word', \n",
    "                               tokenizer=lem_tknzr_sp, \n",
    "                               stop_words=my_stop_words\n",
    "                              )\n",
    "tfidf_tls_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = '.'\n",
    "tfidf_tls_dfm_spacy = DataFrameMapper(\n",
    "    [\n",
    "        ( 'random_text', tfidf_tls_sp, {'alias':alias})\n",
    "    ], \n",
    "    df_out = True,\n",
    "    input_df = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>._can</th>\n",
       "      <th>._employ</th>\n",
       "      <th>._employee</th>\n",
       "      <th>._take</th>\n",
       "      <th>._vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ._can  ._employ  ._employee    ._take  ._vacation\n",
       "0  0.534046  0.000000    0.379978  0.534046    0.534046\n",
       "1  0.000000  0.574962    0.818180  0.000000    0.000000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_df_sp = tfidf_tls_dfm_spacy.fit_transform(random_df)\n",
    "dtm_df_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

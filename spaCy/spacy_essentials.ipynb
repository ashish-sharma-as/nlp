{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Reference: Credit to Jose Portilla for his NLP course on Udemy where most of this content is learned from. I have reproduced this by understanding the concepts taught and re-writing it to ensure practical understanding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# spaCy visualization\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x25018d43848>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load small and large version of the english language library\n",
    "\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "nlp_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2501d903108>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "nlp_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Pipeline\n",
    "\n",
    "- Using the language library just loaded, spacy will sparse the entire string into separate components known as **tokens** where default token unit is word\n",
    "- Various attributes can also be extracted from these tokens\n",
    "- The above line is actually us loading a **Spacy Model Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x2501bc8d588>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x2501bc8d828>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x2501bc3e0b8>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x2501bc3e208>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x2501bcfd748>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x2501bcfdd48>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sm.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sm.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we run nlp_sm, our text will enter a **processing pipeline** as indicated above that breaks down the text and performs a series of operations as described above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "\n",
    "tmp_txt1 = \"Amazon has a market capatalization of $1.75 trillion.     HQ are in the U.S. Isn't that cool!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a document object by applying our model to our text\n",
    "\n",
    "doc = nlp_sm(tmp_txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document object holds the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Amazon has a market capatalization of $1.75 trillion.     HQ are in the U.S. Isn't that cool!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "\n",
    "- Off a document object you can call the vocabulary that is based on the language library loaded at the start\n",
    "- `doc.vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.vocab)\n",
    "\n",
    "# This implies that the library we loaded ('en_core_web_sm') has a vocabulary of 789 tokens (weird!!!!!!!)\n",
    "# Recall: doc = nlp_sm(<text>); and nlp_sm = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lg = nlp_lg(tmp_txt1)\n",
    "len(doc_lg.vocab) # weird!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** is the very first step of text processing, where all the component parts of the text are split into tokens (normally, words and punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokens are the basic building blocks of a Doc object\n",
    "- Get each token from your text using the tokens attribute on the document object\n",
    "- Individual tokens are parsed and tagged with: POS, dependencies and lemmas\n",
    "- **`doc.tokens`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token attributes** \n",
    "\n",
    "- Token Text                               : {token.text} \n",
    "- Token Position / Index                   : {token.idx} \n",
    "- Token Lemmatization                      : {token.lemma_} \n",
    "- Token Part of speech (POS)               : {token.pos_} \n",
    "- Token POS descr                          : {token.tag_} \n",
    "- Does token contain alphabets / Is string : {token.is_alpha} \n",
    "- Is the token a stop word                 : {token.is_stop} \n",
    "- Is the token the start of a sentence     : {token.is_sent_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to this command to discover more\n",
    "# dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon has a market capatalization of $1.75 trillion.     HQ are in the U.S. Isn't that cool!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of tokens a document has\n",
    "print(doc)\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS (and its attributes)\n",
      "\n",
      "Text            Idx Lemma           POS        POS Descr       Alphabet?  Stop Word? Start of sent?  Entity Type\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Amazon          0   Amazon          PROPN      NNP             1          0          1               ORG       \n",
      "has             7   have            VERB       VBZ             1          1          0                         \n",
      "a               11  a               DET        DT              1          1          0                         \n",
      "market          13  market          NOUN       NN              1          0          0                         \n",
      "capatalization  20  capatalization  NOUN       NN              1          0          0                         \n",
      "of              35  of              ADP        IN              1          1          0                         \n",
      "$               38  $               SYM        $               0          0          0               MONEY     \n",
      "1.75            39  1.75            NUM        CD              0          0          0               MONEY     \n",
      "trillion        44  trillion        NUM        CD              1          0          0               MONEY     \n",
      ".               52  .               PUNCT      .               0          0          0                         \n",
      "                54                  SPACE      _SP             0          0          1                         \n",
      "HQ              58  HQ              PROPN      NNP             1          0          0                         \n",
      "are             61  be              AUX        VBP             1          1          0                         \n",
      "in              65  in              ADP        IN              1          1          0                         \n",
      "the             68  the             DET        DT              1          1          0                         \n",
      "U.S.            72  U.S.            PROPN      NNP             0          0          0               GPE       \n",
      "Is              77  be              AUX        VBZ             1          1          1                         \n",
      "n't             79  n't             PART       RB              0          1          0                         \n",
      "that            83  that            ADV        RB              1          1          0                         \n",
      "cool            88  cool            ADJ        JJ              1          0          0                         \n",
      "!               92  !               PUNCT      .               0          0          0                         \n"
     ]
    }
   ],
   "source": [
    "print('TOKENS (and its attributes)')\n",
    "print('')\n",
    "\n",
    "print(f'{\"Text\":{15}} {\"Idx\":>{3}} {\"Lemma\":{15}} {\"POS\":{10}} {\"POS Descr\":{15}} {\"Alphabet?\":{10}} {\"Stop Word?\":{10}} {\"Start of sent?\":{15}} {\"Entity Type\":{10}}')\n",
    "print('-'*110)\n",
    "for token in doc:\n",
    "    print(f'{token.text:{15}} {token.idx:<{3}} {token.lemma_:{15}} {token.pos_:{10}} {token.tag_:{15}} {token.is_alpha:<{10}} {token.is_stop:<{10}} {token.is_sent_start:<{15}} {token.ent_type_:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Insights`**\n",
    "- Spacy knows that Amazon in this context is a proper noun\n",
    "- Spacy tags trillion as number and is a quantifier of money\n",
    "- Spacy knows that U.S. is a proper noun.\n",
    "- Isn't is split into is and n't because Spacy understands the root verb (is) and the negation (n't) \n",
    "- Extended whitespaces are tokens as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon\n",
      "cool\n",
      "~~~~~~~~~~~~~~~\n",
      "PROPN\n",
      "ADJ\n"
     ]
    }
   ],
   "source": [
    "# You can use indexing to grab tokens individually\n",
    "# Index the document object to get individual tokens\n",
    "print(doc[0])\n",
    "print(doc[-2])\n",
    "\n",
    "print('~~~~~~~~~~~~~~~')\n",
    "\n",
    "# Attributes of specific tokens\n",
    "print(doc[0].pos_)\n",
    "print(doc[-2].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a slice from the document object and you get a `Span`\n",
    "- comes in handy when you are only interested in a part of a article\n",
    "- useful for very large documents\n",
    "- One might think that you can slice the original text, but spacy makes it easier. \n",
    "    - If you want the slice the original text by using words are the indexes, then you first have to form words using regex or something and then use that to slice by words. \n",
    "    - However, if you slice the document object, then span will operate on the tokens that form that document. In the case below the tokens are words so it returns words from index 3 till index 6 (not inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the text provided                           :  <class 'str'>\n",
      "Type of the document object made from text provided :  <class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print('Type of the text provided                           : ', type(tmp_txt1))\n",
    "print('Type of the document object made from text provided : ', type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon has a market capatalization of $1.75 trillion.     HQ are in the U.S. Isn't that cool!\n",
      "Amazon has a market capatalization of $1.75 trillion.     HQ are in the U.S. Isn't that cool!\n"
     ]
    }
   ],
   "source": [
    "print(tmp_txt1)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'zon'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_of_txt1 = tmp_txt1[3:6]\n",
    "print(type(slice_of_txt1))\n",
    "slice_of_txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Amazon has a market capatalization of $1.75 trillion.     HQ are in the U.S. Isn't that cool!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "market capatalization of"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_doc = doc[3:6]\n",
    "print(type(span_doc))\n",
    "span_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "market capatalization of"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_of_doc = doc[3:6]\n",
    "print(type(slice_of_doc))\n",
    "slice_of_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get each sentence from your text using the sents attribute on the document object\n",
    "- **`doc.sents`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second.\n",
      "This is the third\n"
     ]
    }
   ],
   "source": [
    "# Pass a unicode string to the nlp model pipeline object to create a document\n",
    "doc2 = nlp_sm(u\"This is the first sentence. This is the second. This is the third\")\n",
    "\n",
    "for sentence in doc2.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the language model you load, spacy recognizes that certain tokens are organization names (e.g. Tesla), are locations, or related to money or dates, etc.\n",
    "- Named entities add another layer of context\n",
    "- are accessible through the `ents` property of a document object\n",
    "- <font color=green> **`doc.ents`** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS in document:\n",
      "Tesla | to | build | a | factory | in | Toronto | for | $ | 4 | million | \n",
      "\n",
      "----------------------------------------\n",
      "ENTITIES in document:\n",
      "Toronto\n",
      "GPE\n",
      "Countries, cities, states\n",
      "\n",
      "$4 million\n",
      "MONEY\n",
      "Monetary values, including unit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp_sm(u'Tesla to build a factory in Toronto for $4 million')\n",
    "\n",
    "print('TOKENS in document:')\n",
    "for token in doc3:\n",
    "    print(token.text, end = ' | ')\n",
    "\n",
    "print('\\n')\n",
    "print('-'*40)\n",
    "\n",
    "print('ENTITIES in document:')\n",
    "for entity in doc3.ents:\n",
    "    print(entity)\n",
    "    print(entity.label_)\n",
    "    print(str(spacy.explain(entity.label_)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can think of noun chunks as a noun plus the words describing the noun – for example, in Sheb Wooley's 1958 song, a <font color=purple>*one-eyed, one-horned, flying, purple people-eater*</font> would be one long noun chunk.\n",
    "- are basically nouns with some descriptor words attached to them (or just the noun itself if there are no descriptor words)\n",
    "- are accessible through the **`noun_chunks`** property of a document object\n",
    "- **`doc.noun_chunks`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp_sm('Autonomous cars shift insurance liability toward manufacturers')\n",
    "\n",
    "for noun_chunk in doc4.noun_chunks:\n",
    "    print(noun_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-In visualizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- works well within jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5 = nlp_sm(u\"Apple is going to build a U.K. factory for $6 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"05419ab644c9463f937919e979b0fc20-0\" class=\"displacy\" width=\"1010\" height=\"297.0\" direction=\"ltr\" style=\"max-width: none; height: 297.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">build</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">6</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">million</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-0\" stroke-width=\"2px\" d=\"M70,162.0 C70,82.0 200.0,82.0 200.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,164.0 L62,152.0 78,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-1\" stroke-width=\"2px\" d=\"M150,162.0 C150,122.0 195.0,122.0 195.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M150,164.0 L142,152.0 158,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-2\" stroke-width=\"2px\" d=\"M310,162.0 C310,122.0 355.0,122.0 355.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,164.0 L302,152.0 318,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-3\" stroke-width=\"2px\" d=\"M230,162.0 C230,82.0 360.0,82.0 360.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M360.0,164.0 L368.0,152.0 352.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-4\" stroke-width=\"2px\" d=\"M470,162.0 C470,82.0 600.0,82.0 600.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,164.0 L462,152.0 478,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-5\" stroke-width=\"2px\" d=\"M550,162.0 C550,122.0 595.0,122.0 595.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550,164.0 L542,152.0 558,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-6\" stroke-width=\"2px\" d=\"M390,162.0 C390,42.0 605.0,42.0 605.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M605.0,164.0 L613.0,152.0 597.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-7\" stroke-width=\"2px\" d=\"M390,162.0 C390,2.0 690.0,2.0 690.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M690.0,164.0 L698.0,152.0 682.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-8\" stroke-width=\"2px\" d=\"M790,162.0 C790,82.0 920.0,82.0 920.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,164.0 L782,152.0 798,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-9\" stroke-width=\"2px\" d=\"M870,162.0 C870,122.0 915.0,122.0 915.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,164.0 L862,152.0 878,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-05419ab644c9463f937919e979b0fc20-0-10\" stroke-width=\"2px\" d=\"M710,162.0 C710,42.0 925.0,42.0 925.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-05419ab644c9463f937919e979b0fc20-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,164.0 L933.0,152.0 917.0,152.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display to dependency b/w tokens\n",
    "\n",
    "displacy.render(doc5, \n",
    "                style='dep',  # Syntatic dependency\n",
    "                jupyter=True, \n",
    "                options={'distance':80}  # distance b/w token\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the Entity recognizer\n",
    "\n",
    "doc6 = nlp_sm(u\"Over the last quarter, Apple sold nearly 20 thousand iPods for a profit of $6 million\")\n",
    "\n",
    "# Highlight every entity\n",
    "displacy.render(doc6, \n",
    "                style='ent',  # Entity\n",
    "                jupyter=True\n",
    "               )\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outside of jupyter notebook (use .serve() instead of .render())\n",
    "\n",
    "# displacy.serve(doc6, \n",
    "#                 style='ent'\n",
    "#                )\n",
    "\n",
    "# Open browser to 127.0.0.1:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cataloging related words (e.g. boat, boats, boating, boater, etc.)\n",
    "- it essentially chops off letters from the end until the stem is reached\n",
    "- spaCy doesn't include a stemmer, instead, it relies on Lemmatization\n",
    "- To perform stemming, use NLTK (section above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generally seen as much more information than Stemming\n",
    "- Looks at surrounding text to dtermine a given word's part os speech\n",
    "- More informative way of reducing down words, while taking into account how the word is being used in the sentence\n",
    "- **`token.lemma_`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I am a runner running in a race because I love to run since I ran today"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lem1 = nlp_sm('I am a runner running in a race because I love to run since I ran today')\n",
    "doc_lem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "---------------------------------------------------------------\n",
      "I               I               PRON         4690420944186131903    \n",
      "am              be              AUX          10382539506755952630   \n",
      "a               a               DET          11901859001352538922   \n",
      "runner          runner          NOUN         12640964157389618806   \n",
      "running         run             VERB         12767647472892411841   \n",
      "in              in              ADP          3002984154512732771    \n",
      "a               a               DET          11901859001352538922   \n",
      "race            race            NOUN         8048469955494714898    \n",
      "because         because         SCONJ        16950148841647037698   \n",
      "I               I               PRON         4690420944186131903    \n",
      "love            love            VERB         3702023516439754181    \n",
      "to              to              PART         3791531372978436496    \n",
      "run             run             VERB         12767647472892411841   \n",
      "since           since           SCONJ        10066841407251338481   \n",
      "I               I               PRON         4690420944186131903    \n",
      "ran             run             VERB         12767647472892411841   \n",
      "today           today           NOUN         11042482332948150395   \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Token Text\":{15}} {\"Token Lemma_\":{15}} {\"Token POS\":{12}} {\"Token Lemma Hash Ref\":{23}} ')\n",
    "print('-'*63)\n",
    "\n",
    "for token in doc_lem1:\n",
    "    print(f'{token.text:{15}} {token.lemma_:{15}} {token.pos_:{12}} {token.lemma:<{23}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to shows lemmas for other documents\n",
    "\n",
    "def show_lemmas(text):\n",
    "    \"\"\"\n",
    "    Helper function to print lemmas of words(tokens) of a provided text\n",
    "    I/P:\n",
    "        - text(NLP document object): NLP doc object\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'{\"Token Text\":{15}} {\"Token Lemma_\":{15}} {\"Token POS\":{12}} {\"Token Lemma Hash Ref\":{23}} ')\n",
    "    print('-'*65)\n",
    "    \n",
    "    for token in text:\n",
    "        print(f'{token.text:{15}} {token.lemma_:{15}} {token.pos_:{12}} {token.lemma:<{23}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I saw the movie Saw yesterday while watching over my dog. You've seen it?"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lem2 = nlp_sm('I saw the movie Saw yesterday while watching over my dog. You\\'ve seen it?')\n",
    "doc_lem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text      Token Lemma_    Token POS    Token Lemma Hash Ref    \n",
      "-----------------------------------------------------------------\n",
      "I               I               PRON         4690420944186131903    \n",
      "saw             see             VERB         11925638236994514241   \n",
      "the             the             DET          7425985699627899538    \n",
      "movie           movie           NOUN         18213940162184454424   \n",
      "Saw             Saw             PROPN        11380715829964476340   \n",
      "yesterday       yesterday       NOUN         1756787072497230782    \n",
      "while           while           SCONJ        1039541750886098100    \n",
      "watching        watch           VERB         2054481287215635300    \n",
      "over            over            ADP          5456543204961066030    \n",
      "my              my              PRON         227504873216781231     \n",
      "dog             dog             NOUN         7562983679033046312    \n",
      ".               .               PUNCT        12646065887601541794   \n",
      "You             you             PRON         7624161793554793053    \n",
      "'ve             've             AUX          2989924464908128948    \n",
      "seen            see             VERB         11925638236994514241   \n",
      "it              it              PRON         10239237003504588839   \n",
      "?               ?               PUNCT        8205403955989537350    \n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc_lem2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- really common english words that dont really provide any additional information\n",
    "- scpaCy holds a built-in list of English stop words (~320 stopwords)\n",
    "- a really common step in text preprocessing NLP pipelines is to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "\n",
      "{'front', 'anywhere', 'top', 'from', 'throughout', 'if', 'only', 'above', 'much', 'whence', 'back', 'namely', 'well', 'across', \"'ve\", 'just', 'own', '‘re', 'twelve', 'whoever', 'almost', 'forty', 'however', 'whither', 'or', 'out', 'again', 'more', 'your', 'most', 'than', 'will', 'elsewhere', 'mine', 'and', 'them', \"'d\", 'latterly', 'ten', 'too', 'four', 'at', 'either', 'n’t', 'whereas', 'towards', 'third', 'see', 'show', 'formerly', 'could', 'do', 'be', 'on', 'all', 'her', 'enough', 'nine', 'although', 'beyond', '‘ll', 'any', 'thence', 'always', 'beside', 'whom', 'really', 'with', 'yourself', 'keep', 'not', 'became', 'us', 'its', 'already', 'often', 'a', 'being', 'him', 'serious', 'several', 'get', 'how', 'various', '’m', 'side', 'yourselves', 'thereupon', 'among', 'bottom', 'every', 'are', 'part', 'nobody', 'anything', 'when', 'whole', 'did', '’ll', 'for', 'whereby', 'eight', 'hereby', 'another', 'last', 'upon', 'nevertheless', 'why', 'i', 'meanwhile', 'were', 'while', 'via', \"'ll\", 'everything', 'less', 'there', 'toward', 'as', 'these', 'whereafter', '’s', 'everywhere', 'you', 'along', 'very', 'it', 'few', 'neither', 'until', 'nowhere', 'noone', 'six', 'something', \"n't\", 'further', 'though', 'thru', 'between', 'still', 'twenty', 'been', '’re', 'empty', 'yours', 'anyone', 'everyone', 'beforehand', 'someone', 'whether', \"'m\", 'give', 'behind', 'seem', 'who', 'an', 'per', 'before', 'latter', 'becoming', 'hence', 'hereupon', 'into', 'afterwards', 'was', 'would', 'else', 'ours', 'otherwise', 'amount', 'nothing', 'also', 'sometimes', 'regarding', 'sometime', 'has', 'within', 'herself', 'anyway', 'done', 'some', 'where', 'thereby', 'hundred', 'of', 'next', 'around', 'this', 'used', 'anyhow', '‘ve', 'whenever', 'after', 'same', 'whose', 'themselves', 'we', 'here', 'full', '‘s', 'whatever', 'even', 'through', 'alone', 'she', 'fifty', 'somehow', 'rather', 'such', 'might', 'since', 'two', 'wherein', 'except', 'due', 're', 'ourselves', 'itself', 'because', '‘m', 'one', 'besides', 'yet', 'made', 'therein', 'least', 'below', 'should', 'never', 'five', 'seems', 'the', 'perhaps', 'so', 'many', 'no', 'myself', 'please', 'those', 'is', 'both', 'his', 'thus', 'what', 'about', 'become', \"'re\", 'by', 'me', 'put', 'had', 'that', 'make', '‘d', 'becomes', 'am', 'wherever', 'cannot', 'amongst', 'himself', 'eleven', 'indeed', 'during', 'now', 'their', 'up', 'sixty', 'unless', 'name', 'others', '’d', 'go', 'seeming', 'does', 'to', 'n‘t', 'say', 'but', 'take', 'have', 'none', 'which', 'my', 'in', 'moreover', 'doing', 'former', 'once', 'whereupon', 'can', 'off', 'under', 'over', 'onto', 'ca', 'hereafter', 'they', 'hers', 'he', 'somewhere', 'first', 'together', 'mostly', 'thereafter', 'must', 'using', 'call', 'fifteen', 'three', 'seemed', 'other', '’ve', 'herein', 'then', 'quite', 'ever', \"'s\", 'move', 'therefore', 'our', 'without', 'may', 'against', 'nor', 'down', 'each'}\n"
     ]
    }
   ],
   "source": [
    "# spaCy default stopwords\n",
    "\n",
    "print(f'Number of stop words: {len(nlp_lg.Defaults.stop_words)}')\n",
    "print(\"\")\n",
    "print(nlp_lg.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lexeme.Lexeme object at 0x000002501DA7DF48>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if a word is a stop word by calling nlp vocab\n",
    "\n",
    "print(nlp_lg.vocab['is'])\n",
    "print(nlp_lg.vocab['is'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lexeme.Lexeme object at 0x000002501D8F9908>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(nlp_lg.vocab['umbrella'])\n",
    "print(nlp_lg.vocab['umbrella'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of existing stop words :  326\n",
      "Is btw a stop word?           :  False\n",
      "Length of existing stop words :  327\n",
      "Is btw a stop word?           :  True\n"
     ]
    }
   ],
   "source": [
    "# ADD you own custom stopword\n",
    "\n",
    "print('Length of existing stop words : ', len(nlp_lg.Defaults.stop_words))\n",
    "print('Is btw a stop word?           : ', nlp_lg.vocab['btw'].is_stop)\n",
    "\n",
    "nlp_lg.Defaults.stop_words.add('btw')\n",
    "nlp_lg.vocab['btw'].is_stop = True\n",
    "\n",
    "print('Length of existing stop words : ', len(nlp_lg.Defaults.stop_words))\n",
    "print('Is btw a stop word?           : ', nlp_lg.vocab['btw'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of existing stop words :  327\n",
      "Is btw a stop word?           :  True\n",
      "Length of existing stop words :  326\n",
      "Is btw a stop word?           :  False\n"
     ]
    }
   ],
   "source": [
    "# REMOVE you own custom stopword\n",
    "\n",
    "print('Length of existing stop words : ', len(nlp_lg.Defaults.stop_words))\n",
    "print('Is btw a stop word?           : ', nlp_lg.vocab['btw'].is_stop)\n",
    "\n",
    "nlp_lg.Defaults.stop_words.remove('btw')\n",
    "nlp_lg.vocab['btw'].is_stop = False\n",
    "\n",
    "print('Length of existing stop words : ', len(nlp_lg.Defaults.stop_words))\n",
    "print('Is btw a stop word?           : ', nlp_lg.vocab['btw'].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'LOWER': 'solarpower'}],\n",
       " [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}],\n",
       " [{'LOWER': 'solar'}, {'LOWER': 'power'}],\n",
       " [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP': '*'}, {'LOWER': 'power'}]]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand the patterns that you're looking to match\n",
    "# and code their patterns to feed to the matcher\n",
    "# Each different pattern is a list of dictionaries,\n",
    "# with each dictionary pertaining to how the token \n",
    "# within the pattern is to be captured.\n",
    "\n",
    "# if case: \"SolarPower\" / \"solarpower\" / \"Solarpower\"\n",
    "# Read: the lower case of the token = solarpower\n",
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "\n",
    "# if case: \"Solar-Power\" / \"solar-power\" / \"Solar-power\"\n",
    "# Read: \n",
    "    # the lower case of the 1st token = solar\n",
    "    # the 2nd token (i.e., the token after the 1st, is a punctuation)\n",
    "    # the lower case of the 3rd token = power\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
    "\n",
    "# if case: \"Solar Power\" / \"solar power\" / \"Solar power\"\n",
    "    # We dont have anything in b/w to take care of the\n",
    "    # space since 1 single space is not a token\n",
    "    # in spacy\n",
    "# Read: \n",
    "    # the lower case of the 1st token = solar\n",
    "    # the lower case of the next token = power\n",
    "    # We leave out a whitespace since a single whitespace\n",
    "    # is not a token\n",
    "pattern3 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
    "\n",
    "# if case: \"solar------power\" / \"solar*@#@power\" / \"solar <any amount of punctuation\" power\"\n",
    "pattern4 = [{'LOWER': 'solar'}, \n",
    "            {'IS_PUNCT': True, \n",
    "             'OP':'*'},  # Optional - zero or more times\n",
    "            {'LOWER': 'power'}]\n",
    "\n",
    "# Putting together all patterns to match for\n",
    "patterns = [pattern1, pattern2, pattern3, pattern4]\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SolarPower\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "solar\n",
      "-\n",
      "power\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Solar\n",
      "power\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Solar---$%power\n"
     ]
    }
   ],
   "source": [
    "# Showing tokens for each of the cases below for better \n",
    "# understanding of how the patterns were defined\n",
    "doc_for_mat1 = nlp_lg(u'SolarPower')\n",
    "doc_for_mat2 = nlp_lg(u'solar-power')\n",
    "doc_for_mat3 = nlp_lg(u'Solar power')\n",
    "doc_for_mat4 = nlp_lg(u'Solar---$%power')\n",
    "\n",
    "for token in doc_for_mat1:\n",
    "    print(token)\n",
    "    # Since only 1 token here, pattern1 has only 1 dictionary\n",
    "    # in the list\n",
    "\n",
    "print('~'*30)\n",
    "for token in doc_for_mat2:\n",
    "    print(token)\n",
    "    # 3 tokens here, so 3 dictionaries for pattern2\n",
    "\n",
    "print('~'*30)\n",
    "for token in doc_for_mat3:\n",
    "    print(token)\n",
    "    # 2 tokens here, so 2 dictionaries for pattern3\n",
    "    # 1 whitespace in b/w does not for a token\n",
    "\n",
    "print('~'*30)\n",
    "for token in doc_for_mat4:\n",
    "    print(token)\n",
    "    # 1 token then any punctuation as separate tokens then another token for power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.matcher.matcher.Matcher at 0x25060535e58>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the matcher object\n",
    "\n",
    "matcher = Matcher(nlp_lg.vocab)\n",
    "matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name our matcher (we could have many matchers)\n",
    "# We do this by adding to the matchers \n",
    "# We'll call the above \"Solarpower_Matcher\"\n",
    "\n",
    "matcher.add('Solarpower_Matcher', patterns)\n",
    "\n",
    "# So now, 3 patterns (to check for) have been added to a\n",
    "# matcher object we've called Solarpower_Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Solar Power industry grows as solarpower increases. Solar-power is good. solar ---$% power"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test the matcher\n",
    "\n",
    "# Create a document\n",
    "doc_mat = nlp_lg(u\"The Solar Power industry grows as solarpower increases. Solar-power is good. solar ---$% power\")\n",
    "doc_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 The\n",
      " 1 Solar\n",
      " 2 Power\n",
      " 3 industry\n",
      " 4 grows\n",
      " 5 as\n",
      " 6 solarpower\n",
      " 7 increases\n",
      " 8 .\n",
      " 9 Solar\n",
      "10 -\n",
      "11 power\n",
      "12 is\n",
      "13 good\n",
      "14 .\n",
      "15 solar\n",
      "16 ---$%\n",
      "17 power\n"
     ]
    }
   ],
   "source": [
    "# Get all tokens of the document \n",
    "\n",
    "for i, token in enumerate(doc_mat):\n",
    "    print(f'{i:{2}} {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the results of the matcher in a variable\n",
    "\n",
    "matches_found = matcher(doc_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9730578706714751770, 1, 3),\n",
       " (9730578706714751770, 6, 7),\n",
       " (9730578706714751770, 9, 12)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuples of (match_id, start position of token (inclusive), stop position of token (exclusive)\n",
    "\n",
    "matches_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9730578706714751770 Solarpower_Matcher   1   3 Solar Power\n",
      "9730578706714751770 Solarpower_Matcher   6   7 solarpower\n",
      "9730578706714751770 Solarpower_Matcher   9  12 Solar-power\n"
     ]
    }
   ],
   "source": [
    "# Printing above in a different way\n",
    "\n",
    "for match_id, start_pos, end_pos in matches_found:\n",
    "    string_id = nlp_lg.vocab.strings[match_id]  # get string representation\n",
    "    span = doc_mat[start_pos : end_pos]  # get the matched span\n",
    "    print(f'{match_id} {string_id} {start_pos:{3}} {end_pos:{3}} {span.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove from matcher if you dont want to detect anymore\n",
    "\n",
    "# matcher.remove('Solarpower_Matcher')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
